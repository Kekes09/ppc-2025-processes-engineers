# Решетка тор

- Студент: Лазарева Анна Анатольевна, группа 3823Б1ПР1
- Технология: MPI + SEQ
- Вариант: 9

## 1. Введение

Топология «Решётка-тор» является одной из наиболее распространённых топологий в параллельных вычислительных системах. Она обеспечивает регулярную структуру соединений между процессорами и позволяет эффективно маршрутизировать данные между любыми двумя узлами сети. Данная топология широко используется в суперкомпьютерах и распределённых вычислительных системах.

В данной работе реализован алгоритм маршрутизации данных между произвольными узлами двумерной торовой решётки с использованием стандарта Message Passing Interface (MPI).

## 2. Постановка задачи

**Определение задачи:** Дана сеть процессов, организованная в виде двумерной торовой решётки. Необходимо передать данные от узла-источника к узлу-получателю по кратчайшему пути, используя только соседние соединения в топологии тора.

**Топология «Решётка-тор»:**
- Процессы организованы в двумерную решётку размером rows × cols, где rows × cols = p (общее число процессов)
- Каждый процесс соединён с 4 соседями: верхним, нижним, левым и правым
- Соединения замкнуты в кольцо по обоим измерениям (тороидальная топология):
  - Левый край соединён с правым
  - Верхний край соединён с нижним

**Формат входных данных:**
- `source` — ранг процесса-источника (0 ≤ source < p)
- `dest` — ранг процесса-получателя (0 ≤ dest < p)
- Входной вектор:  `[source, dest, data[0], data[1], ..., data[n-1]]`

**Формат выходных данных:**
- На процессе-получателе: `[data[0], ..., data[n-1], path_size, path[0], path[1], ..., path[k-1]]`
- На остальных процессах: пустой вектор

**Ограничения:**
- Количество процессов: 1 ≤ p ≤ 256
- Индексы процессов: 0 ≤ source, dest < p
- Размер данных: минимум 1 элемент
- Элементы данных: любые 32-битные целые числа со знаком

**Пример:**
Топология 2×4 (8 процессов):
[0] — [1] — [2] — [3]
 |     |     |     |
[4] — [5] — [6] — [7]
(края замкнуты в тор)

Вход: source=0, dest=6, data=[10, 20, 30]
Путь: 0 → 1 → 2 → 6 (или 0 → 4 → 5 → 6)
Выход на процессе 6: [10, 20, 30, 4, 0, 1, 2, 6]


## 3. Описание базового алгоритма (последовательная версия)

**Шаги алгоритма:**
1. **Разбор входных данных:** Извлечение `source`, `dest` и вектора данных из входного массива
2. **Валидация:** Проверка корректности индексов и наличия данных
3. **Формирование результата:** 
   - Копирование данных в выходной вектор
   - Добавление информации о пути (упрощённо: [`source`, `dest`])
4. **Возврат:** Выходной вектор с данными и путём

**Сложность:**
- Время: O(n), где n — размер данных (простое копирование)
- Память: O(n) для данных + O(1) для пути

**Примечание:** Последовательная версия моделирует прямую передачу данных без реальной маршрутизации, так как все данные находятся в одном адресном пространстве.


## 4. Схема распараллеливания

**Топология «Решётка-тор»:**
**Построение решётки:**
Для p процессов строится решётка rows × cols, где:
- rows — наибольший делитель p, не превышающий √p
- cols = p / rows

Пример для p=12:
rows = 3, cols = 4

[0] — [1] — [2] — [3]
 |     |     |     |
[4] — [5] — [6] — [7]
 |     |     |     |
[8] — [9] — [10]— [11]

**Преобразование координат:**
row = rank / cols;
col = rank % cols;

rank = ((row % rows + rows) % rows) * cols + ((col % cols + cols) % cols);

**Алгоритм маршрутизации**

**Стратегия: Dimension-Order Routing (XY-routing)**
Данные передаются сначала по горизонтали (ось X), затем по вертикали (ось Y). Это гарантирует отсутствие deadlock'ов.

**Выбор направления:*

```cpp
int ShortestDirection(int from, int to, int size) {
    if (from == to) return 0;
    
    int forward = ((to - from) + size) % size;   
    int backward = ((from - to) + size) % size;  
    
    return (forward <= backward) ? 1 : -1;
}
```

**Вычисление следующего узла:*
```cpp
int ComputeNextNode(int current, int dest) {
    
    if (curr_col != dest_col) {
        int dir = ShortestDirection(curr_col, dest_col, cols_);
        return CoordsToRank(curr_row, curr_col + dir);
    }
    
    if (curr_row != dest_row) {
        int dir = ShortestDirection(curr_row, dest_row, rows_);
        return CoordsToRank(curr_row + dir, curr_col);
    }
    return -1;  
}
```

**Схема передачи данных**

**Модель: Store-and-Forward**
Каждый промежуточный узел:
1. Получает полный пакет данных от предыдущего узла
2. Сохраняет данные во внутреннем буфере
3. Пересылает данные следующему узлу на пути

**MPI-операции:**
- MPI_Bcast — рассылка source и dest всем процессам для вычисления пути
- MPI_Send / MPI_Recv — point-to-point передача данных по маршруту

## 5. Детали реализации

### Структура кода

**Файлы:**
- `common/include/common.hpp` - определение типов данных
- `seq/include/ops_seq.hpp`, `seq/src/ops_seq.cpp` - последовательная реализация
- `mpi/include/ops_mpi.hpp`, `mpi/src/ops_mpi.cpp` - параллельная реализация
- `tests/functional/main.cpp` - функциональные тесты
- `tests/performance/main.cpp` - тесты производительности

**Классы:**
- `LazarevaATorusGridSEQ` - последовательная версия (SEQ)
- `LazarevaATorusGridMPI` - параллельная версия (MPI)

**Методы (одинаковы для обеих реализаций):**
- `ValidationImpl()` - проверка корректности входа
- `PreProcessingImpl()` - подготовка данных
- `RunImpl()` - основная логика (последовательная или MPI).
- `PostProcessingImpl()` - проверка корректности выхода.

### Важные решения при реализации

**Выбор размеров решётки:**
```cpp
rows_ = static_cast<int>(std::sqrt(static_cast<double>(world_size_)));
while (rows_ > 0 && world_size_ % rows_ != 0) {
    rows_--;
}
cols_ = world_size_ / rows_;
```
Это обеспечивает максимально «квадратную» решётку для минимизации диаметра.

**Обработка граничных случаев:**
- source == dest: Данные не передаются, путь содержит один элемент
- world_size == 1: Вырожденный случай, эквивалентен SEQ
- Некратное число процессов: Решётка может быть прямоугольной (например, 2×3 для 6 процессов)


## 6. Экспериментальное окружение

### 6.1 Конфигурация оборудования

- **CPU:** AMD Ryzen 9 7940HS w/ Radeon 780M Graphics
- **Ядра:** 8 физических ядер (16 логических потоков)
- **ОЗУ:** 8 ГБ DDR4
- **ОС:** WSL Ubuntu 24.04.3 LTS (Linux kernel 6.x)

### 6.2 Программный инструментарий

- **Компилятор:** g++ 11.4.0 
- **Тип сборки:** Release
- **Стандарт C++:** C++20

### 6.3 Тестовое окружение

```bash

PPC_NUM_PROC=2,4,8,16

```

### 6.4 Генерация тестовых данных

**Функциональные тесты:**
- 7 тестовых случаев с различными размерами данных: 1, 3, 5, 7, 10, 50, 100 элементов
- Проверка передачи между различными парами узлов
- Проверка передачи между различными парами узлов
**Тесты производительности:**
- Размер данных: 10,000,000 элементов
- Маршрут: от процесса 0 до процесса (p-1) — максимальное расстояние

## 7. Результаты

### 7.1 Проверка корректности

**Результаты тестирования:**Все функциональные тесты пройдены
**Валидация:**
- Данные на получателе идентичны исходным
- Путь начинается с source и заканчивается dest
- Длина пути оптимальна (кратчайшее расстояние в торе)
- Результаты MPI совпадают с SEQ для случая source=dest=0

### 7.1 Результаты производительности

**Характеристики задачи:**
- Размер данных: 10,000,000 элементов
- Количество процессов: 8
- Топология: 2×4 (или 4×2)
- Маршрут: 0 → 7 (максимальное расстояние)

| Mode         | Count | Time, s | Speedup | Efficiency |
|--------------|-------|---------|---------|------------|
| SEQ          | 1     | 0.1845  | 1.00    | N/A        |
| MPI pipeline | 8     | 0.1341  | 1.38    | 34.5%      |
| MPI task_run | 8     | 0.1415  | 1.28    | 32.0%      |


## 8. Выводы

### 8.1 Что сработало хорошо

**Корректная реализация топологии тор**
- Правильное вычисление соседей с учётом замыкания краёв
- Оптимальный выбор размеров решётки для произвольного числа процессов

**Эффективный алгоритм маршрутизации**
- XY-routing гарантирует кратчайший путь
- Отсутствие deadlock'ов благодаря dimension-order routing
- Корректная работа для всех пар source/dest

**Надёжная передача данных**
- Протокол с предварительной отправкой размера
- Данные передаются только по пути, а не всем процессам
- Корректная обработка граничных случаев
- Все функциональные тесты пройдены

 ### 8.2 Ограничения и проблемы

**Линейная зависимость от длины пути**
- Время передачи: T = k × (T_send + T_recv + T_copy)
- Для длинных путей задержка значительна

## 9. Источники

1. Лекции Сысоева А.В.
2. Материалы курса, ppc-2025-processes-engineers
https://github.com/learning-process/ppc-2025-processes-engineers

## 10. Приложение

### MPI-реализация(ключевой алгоритм)

```cpp
bool LazarevaATorusGridMPI::RunImpl() {
  int source = 0;
  int dest = 0;

  if (rank_ == 0) {
    const auto &input = GetInput();
    source = input[0];
    dest = input[1];
  }

  MPI_Bcast(&source, 1, MPI_INT, 0, MPI_COMM_WORLD);
  MPI_Bcast(&dest, 1, MPI_INT, 0, MPI_COMM_WORLD);

  std::vector<int> path = ComputeFullPath(source, dest);
  int path_size = static_cast<int>(path.size());

  auto it = std::ranges::find(path, rank_);
  bool on_path = (it != path.end());
  int my_index = on_path ? static_cast<int>(std::distance(path.begin(), it)) : -1;

  std::vector<int> recv_data;

  if (source == dest) {
    if (rank_ == source) {
      const auto &input = GetInput();
      recv_data.assign(input.begin() + 2, input.end());
    }
  } else if (rank_ == source) {
    const auto &input = GetInput();
    recv_data.assign(input.begin() + 2, input.end());
    SendData(recv_data, path[1]);
  } else if (on_path) {
    int prev_node = path[my_index - 1];
    recv_data = ReceiveData(prev_node);

    if (rank_ != dest && (my_index + 1) < path_size) {
      SendData(recv_data, path[my_index + 1]);
    }
  }

  if (rank_ == dest) {
    GetOutput() = std::move(recv_data);
    GetOutput().push_back(path_size);
    GetOutput().insert(GetOutput().end(), path.begin(), path.end());
  } else {
    GetOutput().clear();
  }

  return true;
}

```

