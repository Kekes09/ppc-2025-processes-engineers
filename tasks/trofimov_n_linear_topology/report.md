# Линейная топология передачи данных (MPI / SEQ)

* Студент: Трофимов Никита, группа 3823Б1ПР4
* Технология: MPI, SEQ
* Вариант: 6

---

## 1. Введение

В данной работе рассматривается задача организации виртуальной топологии передачи данных между процессами. Необходимо реализовать линейную топологию, при которой данные передаются последовательно от одного процесса к другому через промежуточные процессы с использованием MPI.

Целью работы является изучение принципов построения виртуальных топологий, анализ накладных расходов межпроцессного взаимодействия и сравнение параллельной MPI-версии с последовательной реализацией (SEQ).

---

## 2. Постановка задачи

Реализовать виртуальную линейную топологию передачи данных между `P` процессами. Передача данных должна обеспечивать возможность отправки значения от любого выбранного процесса `source` к любому целевому процессу `target`.

**Входные данные:**

* `source` — ранг исходного процесса
* `target` — ранг целевого процесса
* `value` — целочисленное значение для передачи

**Выходные данные:**

* Переданное целочисленное значение, доступное всем процессам после завершения вычислений

**Ограничения:**

* `0 ≤ source, target < P`
* `value ≥ 0`

---

## 3. Базовый алгоритм (последовательный)

В SEQ-версии алгоритма нет параллельного взаимодействия. Выходное значение присваивается равным входному (`value`). Последовательная версия служит эталоном для проверки корректности MPI-реализации. Для имитации вычислительной нагрузки используется функция:

```cpp
inline void Work(int n) {
  volatile int acc = 0;
  const int iters = n * 1000;
  for (int i = 0; i < iters; ++i) {
    acc += i % 13;
    acc ^= acc << 1;
    acc += acc >> 3;
  }
}
```

Она позволяет создать нагрузку, необходимую для корректной оценки производительности MPI-версии.

---

## 4. Схема распараллеливания

MPI-версия реализует линейную топологию следующим образом:

* Каждый процесс соответствует одному рангу в `MPI_COMM_WORLD`.
* Значение передаётся от `source` к `target` через все промежуточные ранги.
* Направление передачи определяется автоматически: вперёд или назад по линейной последовательности рангов.
* Передача осуществляется блокирующими вызовами `MPI_Send` и `MPI_Recv`.
* После получения значения целевым процессом выполняется рассылка результата всем процессам с помощью `MPI_Bcast`.
* Для имитации нагрузки используется функция `Work(value)`, выполняемая на каждом процессе перед отправкой или после получения значения.

Также реализованы специальные случаи:

* `source == target` — значение сразу доступно целевому процессу и рассылается всем.
* `size == 1` — результат сразу присваивается выходному значению без MPI-вызовов.

---

## 5. Детали реализации

* **Структура кода:**

  * `ops_mpi.*` — MPI-версия алгоритма
  * `ops_seq.*` — последовательная версия
  * `common` — описание входных и выходных данных

* Реализована проверка корректности входных данных.

* Минимальный объём передаваемых данных: одно целое число.

* Для perf-тестирования добавлена вычислительная нагрузка через функцию `Work(value)`, которая позволяет создать заметную вычислительную нагрузку для измерений.

---

## 6. Экспериментальная установка

* **Аппаратная платформа:** AMD Ryzen 7 7700
* **Среда выполнения:** локальная вычислительная система
* **Технология:** MPI
* **Инструменты сборки:** C++ компилятор, режим Release
* **Число процессов:** 1, 2 и 4
* **Тестовые данные:** `value = 100000`, `source = 0`, `target = P-1`

Измерения проводились с использованием встроенного механизма perf-тестов.

---

## 7. Результаты и обсуждение

### 7.1 Корректность

Корректность MPI-реализации проверялась с помощью модульных тестов (Google Test). Все тесты показали совпадение выходного значения с SEQ-версией для всех комбинаций `source`, `target` и `value`.

### 7.2 Производительность

Результаты измерений времени выполнения (в секундах) приведены в таблице ниже.  
В качестве базового времени для расчёта ускорения использовано время последовательной версии (SEQ).

| Режим | Число процессов | Время, с | Ускорение | Эффективность |
| ----- | --------------- | -------- | --------- | ------------- |
| SEQ   | 1               | 0.1312   | 1.00      | —             |
| MPI   | 1               | 0.1310   | 1.00      | 100%          |
| MPI   | 2               | 0.2011   | 0.65      | 32%           |
| MPI   | 4               | 0.4342   | 0.30      | 7%            |
| MPI   | 8               | 0.9566   | 0.14      | 2%            |

**Анализ:**

* При увеличении числа MPI-процессов время выполнения алгоритма возрастает, а ускорение становится меньше единицы.
* Уже при двух процессах MPI-версия работает медленнее последовательной реализации, что указывает на доминирование накладных расходов над полезными вычислениями.
* Такое поведение объясняется особенностями линейной топологии передачи данных: значение передаётся последовательно через все промежуточные процессы, что приводит к росту количества блокирующих операций `MPI_Send` и `MPI_Recv`.
* С увеличением числа процессов длина цепочки передачи данных растёт линейно, что вызывает резкое снижение эффективности. При 8 процессах эффективность составляет около 2%.
* Полученные результаты демонстрируют, что для данной задачи и выбранной схемы распараллеливания использование MPI не приводит к выигрышу по времени выполнения.

---

## 8. Выводы

* Реализована линейная виртуальная топология передачи данных с использованием MPI, а также последовательная версия алгоритма.
* Корректно обрабатываются все специальные случаи, включая `source == target` и выполнение на одном процессе (`size == 1`).
* Для оценки производительности используется функция `Work(value)`, позволяющая задать вычислительную нагрузку.
* Экспериментальные результаты показывают, что при увеличении числа процессов производительность MPI-версии снижается из-за значительных накладных расходов на коммуникацию и синхронизацию.
* Линейная топология передачи данных плохо масштабируется и оказывается неэффективной при большом числе процессов.
* Для задач с небольшой вычислительной нагрузкой последовательная реализация является более предпочтительной, тогда как MPI оправдан только при иной схеме распараллеливания и более благоприятном соотношении вычислений и коммуникаций.

---

## 9. Список литературы

1. MPI Forum. *MPI: A Message-Passing Interface Standard*.
2. Gropp W., Lusk E., Skjellum A. *Using MPI: Portable Parallel Programming with the Message-Passing Interface*. Addison-Wesley.
3. Сысоев А. В. Лекции по параллельному программированию.

---

## Приложение А. Фрагменты кода

### A.1 Последовательная версия

```cpp
bool TrofimovNLinearTopologySEQ::RunImpl() {
  const auto in = GetInput();
  Work(in.value); // нагрузка
  GetOutput() = in.value;
  return true;
}
```

### A.2 MPI-версия (отправка и приём)

```cpp
if (rank_ == in.source) {
  Work(in.value); // нагрузка перед отправкой
  result = in.value;
  MPI_Send(&result, 1, MPI_INT, rank_ + step, 0, MPI_COMM_WORLD);
}

for (int r = in.source + step; r != in.target + step; r += step) {
  if (rank_ == r) {
    MPI_Recv(&result, 1, MPI_INT, rank_ - step, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);
    Work(result); // нагрузка после получения
    if (r != in.target) {
      MPI_Send(&result, 1, MPI_INT, rank_ + step, 0, MPI_COMM_WORLD);
    }
  }
}

MPI_Bcast(&result, 1, MPI_INT, in.target, MPI_COMM_WORLD);
```
