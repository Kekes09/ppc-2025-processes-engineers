# Обобщённая передача данных от всех процессов одному (MPI Gather)

- Студент: Лучников Евгений Александрович, группа 3823Б1ПР5
- Технология: SEQ | MPI
- Вариант: 5

## 1. Введение 

- Операция gather представляет собой базовый коммуникационный примитив в стандарте MPI, предназначенный для агрегации данных со всех процессов в единый буфер целевого процесса. - Цель исследования — разработка корректной и масштабируемой реализации данной операции, а также сравнительный анализ её производительности с последовательным аналогом.

## 2. Постановка задачи

- Задача: Реализовать обобщённую передачу данных от всех процессов одному.
- Входные данные: массив data типа std::vector<char>, count - количество элементов на процесс, datatype  тип данных (MPI_INT, MPI_FLOAT,MPI_DOUBLE), root - номер корневого процесса.
- Выходные данные: на корневом процессе - массив, содержащий данные всех процессов, упорядоченные по рангу, на остальных процессах - пустой результат.

## 3. Базовый алгоритм (последовательный) 

* Назначение:
- Собрать блоки данных со всех процессов в единый буфер корневого процесса.

* Аргументы:

- data — вектор данных для отправки (тип std::vector<char>)

- count — размер отправляемого блока от каждого процесса (в элементах)

- datatype — идентификатор типа MPI (MPI_INT, MPI_FLOAT, MPI_DOUBLE)

- root — ранг процесса-получателя

* Возвращаемое значение:

- На процессе root: вектор, содержащий данные от всех процессов, упорядоченные по рангу отправителей

- На других процессах: пустой вектор

## Сложность алгоритма:
- Временная: O(n).
- Пространственная: O(n).

## 4. Схема параллелизации
- В MPI-версии используется древовидная схема сбора данных.

Основные идеи:
- процессы перенумеровываются относительно корня;
- сбор данных происходит по этапам с пронумерованным шагом;
- на каждом этапе часть процессов отправляет накопленные данные процессам-получателям;
- корневой процесс в конце получает данные от всех процессов.

## Коммуникации выполняются с помощью:
- MPI_Send
- MPI_Recv

- Для восстановления порядка данных хранится дополнительный массив рангов, соответствующих каждому блоку данных.

## 5. Детали реализации

## Структура проекта
- `common.hpp` — общие типы данных
- `ops_seq.hpp / ops_seq.cpp` — последовательная версия
- `ops_mpi.hpp / ops_mpi.cpp` — MPI версия
- `functional_tests.cpp` — функциональные тесты
- `performance_tests.cpp` — тесты производительности

## Основные классы
- `LuchnikovETransmFrAllToOneGatherSEQ` — последовательная реализация
- `LuchnikovETransmFrAllToOneGatherMPI` — MPI-реализация 

## Основные методы
- `ValidationImpl()` — проверка корректности входных данных
- `PreProcessingImpl()` — подготовка
- `RunImpl()` — основной алгоритм
- `PostProcessingImpl()` — завершение работы

## Особенности реализации
- Поддержка типов int, float, double
- Корректная работа при size = 1
- Корректная работа при count = 1
- Использование собственного алгоритма TreeGatherImpl
- Отсутствие вызовов MPI_Gather

## Использование памяти
- **SEQ**: O(N)
- **MPI**: O(N) на корне

## Граничные случаи
- Count <= 0 - ошибка валидации
- Некорректный datatype - ошибка
- Root >= size - ошибка
- Один процесс
- Средний корень 
- Большие объёмы данных

## 6. Экспериментальная установка

## Аппаратное обеспечение
- **Процессор**: Intel Core i5-12450H
- **ОЗУ**: 16 ГБ
- **ОС**: Windows 10 Pro

## Программное обеспечение
- **Компилятор**: MSVC 19.36
- **MPI**: MS-MPI 10.0
- **CMake**: 4.1.2
- **Фреймворк тестирования** : Google Test

## Параметры тестирования

**Разные сценарии**: 
- Сбор данных при root = 0
- Минимальный размер данных (count = 1)
- Большие объёмы данных
- Различные типы данных (int, float, double)
- Проверка корректного порядка данных в выходном буфере

**Функциональные тесты**:
- Размер блока данных от 1 до 1000 элементов
- Типы данных: MPI_INT, MPI_FLOAT, MPI_DOUBLE
- Корневой процесс: 0

## Тесты производительности
- Длина: 50000000
- Типы данных: MPI_INT, MPI_FLOAT, MPI_DOUBLE
- Количество процессов MPI: 1, 2, 3, 4
- Режимы выполнения: task_run, pipeline

## 7. Результаты и обсуждение

## 7.1 Корректность

## Все функциональные тесты успешно пройдены
Тесты учитывают: 
- Базовый gather
- Неверные входные данные
- Большие массивы
- SEQ и MPI версии


### 7.2 Производительность

Тестирование проводилось с длиной 50 000 000 

- task_run:

| Режим | Процессов | Время, с | Ускорение | Эффективность |
| ----- | --------: | -------: | --------: | ------------: |
| seq   |         1 |  0.02262 |      1.00 |           N/A |
| mpi   |         1 |  0.28752 |      0.08 |          7.9% |
| mpi   |         2 |  0.57230 |      0.04 |          2.0% |
| mpi   |         3 |  0.96400 |      0.02 |          0.8% |
| mpi   |         4 |  1.29530 |      0.02 |          0.4% |

- task_pipeline:

| Режим | Процессов | Время, с | Ускорение | Эффективность |
| ----- | --------: | -------: | --------: | ------------: |
| seq   |         1 |  0.02231 |      1.00 |           N/A |
| mpi   |         1 |  0.29649 |      0.08 |          7.5% |
| mpi   |         2 |  0.82388 |      0.03 |          1.4% |
| mpi   |         3 |  1.43997 |      0.02 |          0.5% |
| mpi   |         4 |  2.01992 |      0.01 |          0.3% |

## Анализ производительности

Расчёт метрик:
- Ускорение = T_seq / T_mpi
- Эффективность = (Speedup / P) × 100%

## Анализ результатов

**task_run**:
- Реализация на базе MPI демонстрирует значительное замедление по сравнению с последовательной версией при использовании одного процесса. Это объясняется издержками инициализации MPI, организацией межпроцессных обменов и самой архитектурой алгоритма SEQ.

- При увеличении числа процессов до 2, 3 и 4 время выполнения продолжает возрастать, что свидетельствует о доминировании коммуникационных затрат над полезной вычислительной работой.

* Режим task_pipeline:

- На одном процессе MPI-версия также уступает последовательной реализации, поскольку дополнительные этапы конвейерной обработки не компенсируют накладные расходы.

- Рост числа процессов приводит к дальнейшему увеличению времени выполнения из-за возрастающего количества коммуникационных операций и точек синхронизации.

* Вывод:
- Режим task_run демонстрирует более стабильное поведение по сравнению с pipeline, однако не обеспечивает ускорения относительно последовательной реализации.

- Режим pipeline оказался наименее эффективным для данной задачи ввиду высоких накладных расходов на межпроцессное взаимодействие.

- Для операции gather, ориентированной на передачу данных к одному процессу, коммуникационные издержки существенно превышают потенциальный выигрыш от параллелизма.

## 8. Общие выводы
- Реализованы последовательная и MPI-версии алгоритма обобщённой передачи данных от всех процессов к одному (операция gather).

- Корректность реализации подтверждена функциональным тестированием для различных типов и объёмов входных данных.

- С увеличением числа процессов накладные расходы на коммуникацию и синхронизацию начинают доминировать над полезными вычислениями, что ограничивает масштабируемость решения.


## Приложение

### Фрагменты кода

**Последовательная версия (SEQ):**

```cpp
bool LuchnikovETransmFrAllToOneGatherSEQ::RunImpl() {
  const auto &input = GetInput();

  GetOutput() = input.data;

  return true;
}
```

**Параллельная версия (MPI):**

```cpp
bool LuchnikovETransmFrAllToOneGatherMPI::RunImpl() {
  int rank = 0;
  int size = 1;
  MPI_Comm_rank(MPI_COMM_WORLD, &rank);
  MPI_Comm_size(MPI_COMM_WORLD, &size);

  const auto &input = GetInput();

  int type_size = GetTypeSize(input.datatype);

  std::vector<char> recv_buffer;
  if (rank == input.root) {
    recv_buffer.resize(input.count * size * type_size);
  }

  int result =
      TreeGatherImpl(input.data.data(), input.count, input.datatype, rank == input.root ? recv_buffer.data() : nullptr,
                     input.count, input.datatype, input.root, MPI_COMM_WORLD);

  if (result != MPI_SUCCESS) {
    return false;
  }

  if (rank == input.root) {
    GetOutput() = std::move(recv_buffer);
  } else {
    GetOutput() = std::vector<char>();
  }

  return true;
}
```