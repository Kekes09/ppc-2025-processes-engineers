# Поразрядная сортировка для вещественных чисел (тип double) с простым слиянием
- Студент: Колотухин Александр Дмитриевич, 3823Б1ПР2
- Технология: SEQ, MPI
- Вариант: 20

## 1. Введение
В современных вычислительных системах обработка вещественных чисел двойной точности является важной для научных расчетов, финансового моделирования и машинного обучения. Эффективная сортировка таких данных позволяет ускорить выполнение алгоритмов. В данной работе рассматривается реализация поразрядной сортировки для чисел с плавающей точкой типа `double` с использованием технологии MPI для параллельной обработки.

## 2. Постановка задачи
**Формальная постановка:**

На вход поступает вектор вещественных чисел типа `double` произвольного размера. Требуется отсортировать элементы вектора в порядке возрастания.

**Входные данные:**
- Вектор вещественных чисел типа `double` длины `n`

**Выходные данные:**
- Вектор тех же чисел, отсортированный в порядке возрастания

**Дополнительные требования:**
- Поддержка отрицательных чисел
- Сохранение порядка равных элементов (стабильная сортировка)
- Обработка крайних случаев: пустые массивы и массивы из одного элемента

## 3. Базовый алгоритм (последовательный)
### 3.1 Поразрядная сортировка для double
Последовательный алгоритм состоит из следующих шагов:

1. Преобразование `double` -> `uint64_t`: для сортировки отрицательных чисел используется преобразование:
  - У отрицательных чисел (бит знака = 1) инвертируем все биты
  - У положительных чисел (бит знака = 0) устанавливаем бит знака в 1

2. Поразрядная сортировка `uint64_t` выполняется по 8 бит за итерацию (всего 8 итераций для 64 бит):
  - Для каждой позиции (0-7, 8-15, ..., 56-63 биты):
    - Подсчет количества элементов для каждого значения разряда
    - Построение префиксных сумм
    - Распределение элементов по временному массиву
    - Обратное преобразование `uint64_t` -> `double` в исходные числа

### 3.2 Псевдокод алгоритма
```cpp
void RadixSortDoubles(vector<double>& data) {
    // double -> uint64_t
    vector<uint64_t> keys = TransformDoublesToKeys(data);

    // сортировка
    for (int shift = 0; shift < 64; shift += 8) {
      CountingSortByByte(keys, shift);
    }

    // uint64_t -> double 
    data = transform_keys_to_doubles(keys);
}
```
## 4. Схема распараллеливания
### 4.1 Общая архитектура

1. Распределение данных
    - массив равномерно распределяется между процессами MPI
2. Локальная сортировка
    - каждый процесс сортирует свою часть поразрядно
3. Иерархическое слияние 
    - процессы объединяются парами в бинарном дереве
4. Финализация
    - результат собирается на процессе с `rank = 0` и рассылается

### 4.2 Топология и обмены данными
**Шаг 1: Распределение данных**

```cpp
// Рассылка размера данных
MPI_Bcast(&global_size, 1, MPI_INT, 0, MPI_COMM_WORLD);
// Распределение исходного вектора по процессам
MPI_Scatterv( );
```

**Шаг 2: Иерархическое слияние**
```cpp
int step = 1;
while (step < size) {
    if (rank % (2 * step) == 0) {
        // Получаю данные от партнера
        ReceiveAndMerge(rank + step);
    } else if ((rank - step) % (2 * step) == 0) {
        // Отправляю данные партнеру
        SendData(rank - step);
    }
    step *= 2;
    MPI_Barrier(MPI_COMM_WORLD);
}
```

### 4.3 Диаграмма процесса слияния
```text
    [P0: отсортированный массив]
               /       \
        [P0+P1]         [P2+P3]
        /     \         /     \
    [P0]       [P1] [P2]       [P3]
```

## 5. Детали реализации
### 5.1 Структура кода
```text
kolotukhin_a_merge_sort_doubles
    │
    ├───common
    │   └───include
    │       └───common.hpp — Общие типы данных
    ├───mpi
    │   ├───include
    │   │   └───ops_mpi.hpp — Заголовочный файл параллелльной версии программы
    │   └───src
    │       └───ops_mpi.cpp — Реализация параллельной версии
    ├───seq
    │   ├───include
    │   │   └───ops_seq.hpp — Заголовочный файл последовательной версии программы
    │   └───src
    │       └───ops_seq.cpp — реализация последовательной версии
    └───tests
        ├───functional
        │   └───main.cpp — Тесты оценки функционирования
        └───performance
            └───main.cpp — Тесты оценки производительности
```
### 5.2 Ключевые классы и методы
**Классы**
- `KolotukhinAMergeSortDoublesSEQ`: последовательная реализация
- `KolotukhinAMergeSortDoublesMPI`: параллельная реализация с MPI

**Методы**
- `RadixSortDoubles`: поразрядная сортировка элементов вектора
- `MergeSortedArrays`: слияние отсортированных векторов

### 5.3 Обработка особых случаев
- Пустой вектор -> возвращается пустой вектор
- Один элемент -> возвращается тот же элемент
- Отрицательные числа -> корректно обрабатываются через преобразование битов
- Равные элементы -> cохраняется относительный порядок

### 5.4 Использование памяти
- Локально: `O(n/p)` на процесс, где `n` - общий размер, `p` - число процессов
- Временные массивы: `O(n/p)` для поразрядной сортировки
- Коммуникация: `O(n)` для рассылки итогового результата

### 6. Экспериментальная установка
- Аппаратное обеспечение / ОС 
  - Модель процессора: Intel Core 7 240H (10 ядер, 16 потоков)
  - Оперативная память: 16 ГБ
  - Версия ОС: Windows 11 Home 25H2 (26200.6899)
- Инструменты 
  - Компилятор: MSVC версии 19.44.35217.0 (Visual Studio 2022)
  - Тип сборки: Release
  - MPI: Microsoft MPI 10.1.12498.52
- Окружение
  - Количество процессов: 1, 2, 4, 8, 16, 32
- Данные
  - Размер данных: 1000000 элементов для тестов производительности
  - Генерация тестовых данных

```cpp
// Для функциональных тестов
std::vector<double>{3.5, -2.1, 0.0, 1.1, -3.3, 2.2, -1.4, 5.6}

// Для тестов производительности
for (size_t i = 0; i < 1000000; ++i) {
    double value = static_cast<double>(1000000 - i);
    if (i % 3 == 0) {
        value = -value;
    }
    if (i % 7 == 0) {
        value += 0.5;
    }
    test_data[i] = value;
}
```

## 7. Результаты и обсуждение
### 7.1 Корректность
Корректность реализации проверялась с помощью набора функциональных тестов:
- Тест случайной расстановки: `{3.5, -2.1, 0.0, 1.1, -3.3, 2.2, -1.4, 5.6}` -> `{-3.3, -2.1, -1.4, 0.0, 1.1, 2.2, 3.5, 5.6}`
- Отсортированный массив
- Обратный порядок
- Одинаковые элементы
- Пустой массив
- Один элемент

Все тесты проходят успешно для обеих версий.

## 7.2 Производительность
Результаты performance-тестов (режим `task_run`) для сортиовки 1000000 элементов:

| Версия |  Процессы  |  Время (с)  |  Ускорение  |  Эффективность  |
|:------:|:----------:|:-----------:|:-----------:|:---------------:|
|   seq  |     1      |    0.027    |     1       |        -        |
|   mpi  |     2      |    0.018    |     1.50    |      75.00%     |
|   mpi  |     4      |    0.015    |     1.80    |      45.00%     |
|   mpi  |     8      |    0.018    |     1.50    |      18.75%     |
|   mpi  |    16      |    0.030    |     0.90    |       5.63%     |
|   mpi  |    32      |    0.100    |     0.27    |       0.84%     |

**Анализ результатов:**
- Максимальное ускорение 1.80× достигается при 4 процессах
- 2-4 процесса демонстрируют практическую полезность
- При увеличении числа процессов свыше 4 наблюдается снижение эффективности

## 8. Выводы
### 8.1 Достигнутые результаты:
- Реализация корректно выполняет задачу по сортировке вещественных чисел
- прохождение функционлаьных тестов подтверждает корректность реализации
- Корректная обработка всех случаев (отрицательные числа, пустые массивы)
- Стабильная сортировка с сохранением порядка равных элементов
- MPI-версия имеет ускорение до 1.8× на 4 процессах
- Алгоритм демонстрирует хорошую масштабируемость до 8 процессов

### 8.2 Ограничения:
- Оптимально использовать 2-4 процесса для массивов ~1 млн элементов
- Не рекомендуется использовать больше 8 процессов для средних массивов

**Практическая значимость:** реализация успешно решает задачу сортировки вещественных чисел, демонстрируя практическую полезность для обработки массивов данных в несколько миллионов элементов на многопроцессорных системах. Алгоритм эффективен в научных вычислениях и обработке данных, где требуется сортировка экспериментальных измерений.

### 9. Источники
1. Документация по курсу «Параллельное программирование» / Parallel Programming Course [Электронный ресурс]. — Режим доступа: https://learning-process.github.io/parallel_programming_course/ru/index.html. — Дата обращения: 20.12.2025.
2. Kendall W. MPI Tutorial [Электронный ресурс]. — URL: https://mpitutorial.com. — Дата обращения: 20.12.2025.
3. Сысоев А. В. Курс лекций по параллельному программированию [Электронный ресурс]. — URL: https://source.unn.ru. — Требуется авторизация. — Дата обращения: 20.12.2025.

### Приложение
```cpp
// Преобразование double -> uint64_t
uint64_t double_to_sortable_key(double value) {
    uint64_t u;
    std::memcpy(&u, &value, sizeof(double));

    if (u & 0x8000000000000000ULL) {  // Отрицательное число
        u = ~u;                         // Инвертируем все биты
    } else {                          // Положительное число
        u |= 0x8000000000000000ULL;     // Устанавливаем бит знака
    }

    return u;
}

bool KolotukhinAMergeSortDoublesMPI::RunImpl() {
  int rank = 0;
  int size = 0;
  MPI_Comm_rank(MPI_COMM_WORLD, &rank);
  MPI_Comm_size(MPI_COMM_WORLD, &size);

  const auto &input = GetInput();
  int global_size = static_cast<int>(input.size());
  MPI_Bcast(&global_size, 1, MPI_INT, 0, MPI_COMM_WORLD);

  if (global_size == 0) {
    GetOutput() = std::vector<double>();
    MPI_Barrier(MPI_COMM_WORLD);
    return true;
  }

  int local_size = global_size / size;
  if (rank < global_size % size) {
    local_size++;
  }

  std::vector<int> displs(size, 0);
  std::vector<int> recv_counts(size, 0);

  if (rank == 0) {
    int offset = 0;
    for (int i = 0; i < size; ++i) {
      recv_counts[i] = global_size / size + (i < (global_size % size) ? 1 : 0);
      displs[i] = offset;
      offset += recv_counts[i];
    }
  }

  MPI_Bcast(recv_counts.data(), size, MPI_INT, 0, MPI_COMM_WORLD);
  MPI_Bcast(displs.data(), size, MPI_INT, 0, MPI_COMM_WORLD);

  std::vector<double> local_data(local_size);

  MPI_Scatterv(rank == 0 ? input.data() : nullptr, recv_counts.data(), displs.data(), MPI_DOUBLE,
    local_data.data(), local_size, MPI_DOUBLE, 0, MPI_COMM_WORLD);

  RadixSortDoubles(local_data);

  int step = 1;
  while (step < size) {
    if ((rank % (2 * step)) == 0) {
      int source_rank = rank + step;
      if (source_rank < size) {
        int remote_size = 0;
        MPI_Recv(&remote_size, 1, MPI_INT, source_rank, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);
        if (remote_size > 0) {
          std::vector<double> remote_data(remote_size);
          MPI_Recv(remote_data.data(), remote_size, MPI_DOUBLE, source_rank, 1, MPI_COMM_WORLD, MPI_STATUS_IGNORE);
          local_data = MergeSortedArrays(local_data, remote_data);
        }
      }
    } else if (((rank - step) % (2 * step)) == 0) {
      int dest_rank = rank - step;
      int send_size = static_cast<int>(local_data.size());
      MPI_Send(&send_size, 1, MPI_INT, dest_rank, 0, MPI_COMM_WORLD);
      if (send_size > 0) {
        MPI_Send(local_data.data(), send_size, MPI_DOUBLE, dest_rank, 1, MPI_COMM_WORLD);
      }
      local_data.clear();
    }
    step *= 2;
    MPI_Barrier(MPI_COMM_WORLD);
  }

  if (rank != 0) {
    local_data.resize(global_size);
  }
  MPI_Bcast(local_data.data(), global_size, MPI_DOUBLE, 0, MPI_COMM_WORLD);
  GetOutput() = local_data;
  return true;
}
```
