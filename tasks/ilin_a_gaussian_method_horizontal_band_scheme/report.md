# Метод Гаусса - ленточная горизонтальная схема.

- Студент: Ильин Артемий Александрович, группа 3823Б1ПР1
- Технология: SEQ, MPI
- Вариант: 15

## 1. Введение
Решение систем линейных алгебраических уравнений является фундаментальной задачей вычислительной математики. Для больших разреженных систем использование ленточных матриц позволяет значительно сократить объем памяти и вычислительные затраты. Метод Гаусса с выбором главного элемента обеспечивает устойчивость решения, а параллельная реализация с горизонтальным распределением данных позволяет эффективно использовать вычислительные ресурсы многопроцессорных систем.

Целью данной работы является разработка и анализ последовательной и параллельной реализаций метода Гаусса для ленточных матриц с использованием горизонтальной схемы распределения данных между процессами MPI.

## 2. Постановка задачи
**Описание задачи:** Решение СЛАУ вида Ax = b, где A - ленточная матрица размерности n×n с шириной ленты m, b - вектор правых частей.

**Входные данные:** 
- Целое n - размер матрицы
- Целое m - ширина ленты
- Вектор матричных коэффициентов размером n×m в ленточном формате
- Вектор правых частей размером n

**Выходные данные:** Вектор x размером n - решение системы

**Ограничения:**
- Матрица хранится в ленточном формате: для каждой строки i хранятся только элементы в диапазоне [max(0, i-m+1), i]
- Используется метод Гаусса с выбором главного элемента
- Реализована горизонтальная схема распределения строк матрицы между процессами
- Обеспечена корректная обработка сингулярных систем

## 3. Описание базового алгоритма

```cpp
void ForwardElimination(int n, int m, std::vector<double> &matrix, std::vector<double> &b) {
  for (int k = 0; k < n; ++k) {
    int max_row = FindPivotRow(k, n, m, matrix);
    if (max_row != k) {
      SwapRows(k, max_row, m, matrix, b);
    }
    int diag_idx = m - 1;
    double pivot = matrix[(k * m) + diag_idx];
    if (std::fabs(pivot) < 1e-12) continue;
    for (int i = k + 1; i < std::min(n, k + m); ++i) {
      EliminateRow(i, k, m, matrix, b, pivot);
    }
  }
}
```

**Шаги алгоритма:**

1. **Прямой ход метода Гаусса:**
   - Для каждого элемента k от 0 до n-1:
     - Поиск максимального по модулю элемента в текущем столбце в пределах ленты
     - Перестановка строк для выбора главного элемента
     - Проверка на вырожденность 
     - Исключение переменной из последующих строк в пределах ленты

2. **Обратный ход:**
   - Вычисление решения начиная с последнего уравнения
   - Учет только ненулевых элементов в пределах ленты

**Особенности ленточного формата:**
- Элемент A[i][j] хранится в позиции `matrix[i*m + (m-1 - (i-j))]` при условии |i-j| < m
- Диагональные элементы всегда находятся в позиции `m-1` каждой строки
- Экономия памяти: O(n×m) вместо O(n²)

**Сложность:** O(n×m²), где n - размер системы, m - ширина ленты

## 4. Схема распараллеливания

**Горизонтальная схема распределения данных**

Строки матрицы равномерно распределяются между процессами. Каждый процесс получает непрерывный блок строк:
- Для системы размером `n` и `p` процессов:
- Первые `n % p` процессов получают `⌊n/p⌋ + 1` строк
- Остальные процессы получают `⌊n/p⌋` строк

**Коммуникация между процессами**

**Распределение входных данных:**
```cpp
void BroadcastInputData() {
  MPI_Bcast(&data_.size, 1, MPI_INT, 0, MPI_COMM_WORLD);
  MPI_Bcast(&data_.band_width, 1, MPI_INT, 0, MPI_COMM_WORLD);
  MPI_Bcast(data_.matrix.data(), n_ * band_, MPI_DOUBLE, 0, MPI_COMM_WORLD);
  MPI_Bcast(data_.vector.data(), n_, MPI_DOUBLE, 0, MPI_COMM_WORLD);
}
```

**Поиск главного элемента с редукцией:**
```cpp
double local_max = FindLocalPivotValue(k, local_max_row);
MPI_Allreduce(&local_data, &global_data, 1, MPI_DOUBLE_INT, 
              MPI_MAXLOC, MPI_COMM_WORLD);
```

**Обмен данными при перестановке строк:**
- Если переставляемые строки находятся на разных процессах, то выполняется обмен сообщениями
- Используется MPI_Send/MPI_Recv для передачи строк матрицы и соответствующих элементов вектора b

**Распространение ведущей строки:**
```cpp
void BroadcastPivotData(int pivot_owner, std::vector<double> &pivot_row, 
                       double &pivot_b, int global_max_row) const {
  if (pivot_owner == rank_) {
    std::ranges::copy(row_start, row_start + band_, pivot_row.begin());
    pivot_b = local_vector_[local_pivot_idx];
  }
  MPI_Bcast(pivot_row.data(), band_, MPI_DOUBLE, pivot_owner, MPI_COMM_WORLD);
  MPI_Bcast(&pivot_b, 1, MPI_DOUBLE, pivot_owner, MPI_COMM_WORLD);
}
```

**Сбор результатов:**
```cpp
void GatherAllData(std::vector<double> &recv_matrix, 
                  std::vector<double> &recv_vector) {
  MPI_Gatherv(local_matrix_.data(), local_rows_ * band_, MPI_DOUBLE, 
              recv_matrix.data(), recv_counts.data(), displs.data(), 
              MPI_DOUBLE, 0, MPI_COMM_WORLD);
  MPI_Gatherv(local_vector_.data(), local_rows_, MPI_DOUBLE, 
              recv_vector.data(), vec_counts.data(), vec_displs.data(), 
              MPI_DOUBLE, 0, MPI_COMM_WORLD);
}
```

## 5. Детали реализации

**Структура проекта**
```
ilin_a_gaussian_method_horizontal_band_scheme/
├── common/
│   └── include/
│       └── common.hpp           
├── mpi/
│   ├── include/
│   │   └── ops_mpi.hpp         
│   └── src/
│       └── ops_mpi.cpp         
├── seq/
│   ├── include/
│   │   └── ops_seq.hpp         
│   └── src/
│       └── ops_seq.cpp         
├── tests/
│   ├── functional/
│   │   └── main.cpp            
│   └── performance/
│       └── main.cpp            
└── settings.json
```

**Общие компоненты (common/include/common.hpp)**

Определение структуры для хранения данных:
```cpp
struct MatrixData {
  std::vector<double> matrix;    // Матрица в ленточном формате
  std::vector<double> vector;    // Вектор правых частей
  int size = 0;                  // Размер системы
  int band_width = 0;            // Ширина ленты
};
```

**Последовательная реализация (seq/)**

**ops_seq.cpp - ключевые методы:**

- **FindPivotRow** - поиск главного элемента в пределах ленты:
```cpp
int FindPivotRow(int k, int n, int m, const std::vector<double> &matrix) {
  int max_row = k;
  double max_val = 0.0;
  for (int i = k; i < std::min(n, k + m); ++i) {
    int diag_idx = m - 1 - (i - k);
    if (diag_idx >= 0) {
      double val = std::fabs(matrix[(i * m) + diag_idx]);
      if (val > max_val) {
        max_val = val;
        max_row = i;
      }
    }
  }
  return max_row;
}
```

- **EliminateRow** - исключение переменной с учетом ленточной структуры:
```cpp
void EliminateRow(int i, int k, int m, std::vector<double> &matrix, 
                  std::vector<double> &b, double pivot) {
  int factor_idx = m - 1 - (i - k);
  double factor = matrix[(i * m) + factor_idx] / pivot;
  for (int j = 0; j < m; ++j) {
    int src_idx = j - (i - k);
    if (src_idx >= 0 && src_idx < m) {
      matrix[(i * m) + j] -= factor * matrix[(k * m) + src_idx];
    }
  }
  b[i] -= factor * b[k];
}
```

**MPI реализация (mpi/)**

**ops_mpi.cpp - основные компоненты:**

- **Распределение данных** - горизонтальное распределение строк:
```cpp
void ScatterLocalData() {
  local_matrix_.resize(local_rows_ * band_);
  local_vector_.resize(local_rows_);
  for (int i = 0; i < local_rows_; ++i) {
    int global_row = row_start_ + i;
    std::copy(data_.matrix.begin() + (global_row * band_),
              data_.matrix.begin() + ((global_row + 1) * band_),
              local_matrix_.begin() + (i * band_));
    local_vector_[i] = data_.vector[global_row];
  }
}
```

- **Координация перестановок строк**:
```cpp
void HandleRowSwapLocal(int k_local_idx, int pivot_owner, int global_max_row) {
  if (pivot_owner == rank_) {
    SwapRowsLocally(k_local_idx, pivot_local_idx);
  } else {
    ExchangeRowsWithRemote(k_local_idx, pivot_owner);
  }
}
```

- **Обратный ход** - выполняется на процессе 0 после сбора всех данных:
```cpp
void SolveBackwardSubstitution(const std::vector<double> &full_matrix,
                               const std::vector<double> &full_vector) {
  for (int i = n_ - 1; i >= 0; --i) {
    double sum = 0.0;
    for (int j = i + 1; j < std::min(n_, i + band_); ++j) {
      int idx = band_ - 1 + (j - i);
      if (idx < band_) {
        sum += full_matrix[(i * band_) + idx] * solution_[j];
      }
    }
    int diag_idx = band_ - 1;
    double diag = full_matrix[(i * band_) + diag_idx];
    if (std::fabs(diag) > 1e-12) {
      solution_[i] = (full_vector[i] - sum) / diag;
    } else {
      solution_[i] = 0.0;
    }
  }
}
```

**Тестирование**

**Функциональные тесты (9 тестовых случаев):**
```cpp
const std::array<TestType, 9> kTestParam = {
    std::make_tuple(3, "tiny"),    std::make_tuple(5, "tiny"),
    std::make_tuple(7, "small"),   std::make_tuple(10, "small"),
    std::make_tuple(20, "medium"), std::make_tuple(50, "medium"),
    std::make_tuple(100, "large"), std::make_tuple(8, "singular"),
    std::make_tuple(15, "singular")
};
```

**Тесты производительности:**
- Размер системы: 10,000 уравнений
- Ширина ленты: 100
- Генерация матриц

## 6. Результаты экспериментов

**Окружение:**
- Процессор: 11th Gen Intel(R) Core(TM) i5-1135G7 @ 2.40GHz, 2419 МГц, ядер: 4, логических процессоров: 8
- Архитектура: AMD64
- Ядра: 4
- Оперативная память: 16 GB
- Операционная система: Windows 10 (базовая) / Ubuntu 24.04.3 LTS (сборочная)
- Подсистема: WSL2 (Windows Subsystem for Linux)
- Компилятор: GCC 13.3.0 (Ubuntu 13.3.0-6ubuntu2~24.04)
- MPI реализация: Open MPI 4.1.6
- Тип сборки: Release

**Тестовые данные:**
- Размер системы: 10,000 уравнений
- Ширина ленты: 100 элементов (1% от размера системы)
- Тип матрицы: диагонально преобладающие с случайными элементами

**Методика измерений:**
- Используется значение `task_run` из вывода тестов производительности
- Для MPI реализаций выполнены запуски с 2, 4, 6, и 8 процессами

## 7. Результаты и обсуждение

### 7.1 Корректность
Корректность проверена через 9 функциональных тестов:
- Матрицы малого размера (3-7 уравнений)
- Матрицы среднего размера (10-50 уравнений)
- Матрицы большего размера (100 уравнений)
- Сингулярные и плохо обусловленные системы
- Проверка с различной шириной ленты

Все тесты пройдены с точностью 10^-5, что подтверждает корректность реализации задачи.

### 7.2 Производительность


**Результаты измерений реализаций:**

| Технология   | Кол-во процессов | Время, сек | Ускорение | Эффективность |
|--------------|------------------|------------|-----------|---------------|
| SEQ (оценка) | 1                | 0.250000   | 1.00      | 100.0%        |
| MPI          | 2                | 0.126348   | 1.98      | 99.0%         |
| MPI          | 4                | 0.091230   | 2.74      | 68.5%         |
| MPI          | 6                | 0.081173   | 3.08      | 51.3%         |
| MPI          | 8                | 0.070461   | 3.55      | 44.4%         |

**Расчеты ускорения:**
- SEQ (оценка): 0.250000 сек
- MPI 2 процесса: Speedup = 0.250000 / 0.126348 = 1.98 раза
- MPI 4 процесса: Speedup = 0.250000 / 0.091230 = 2.74 раза
- MPI 6 процессов: Speedup = 0.250000 / 0.081173 = 3.08 раза
- MPI 8 процессов: Speedup = 0.250000 / 0.070461 = 3.55 раза

**Расчет эффективности:**
- MPI 2 процесса: (1.98 / 2) * 100% = 99.0%
- MPI 4 процесса: (2.74 / 4) * 100% = 68.5%
- MPI 6 процессов: (3.08 / 6) * 100% = 51.3%
- MPI 8 процессов: (3.55 / 8) * 100% = 44.4%

**Анализ результатов:**

1. **Высокая эффективность на 2 процессах (99.0%).** Достигнуто идеальное ускорение благодаря хорошему балансу вычислений и минимальным накладным расходам на коммуникации. При двух процессах коммуникационные затраты незначительны по сравнению с объемом вычислений.
2. **Снижение эффективности с ростом числа процессов.** При увеличении числа процессов с 2 до 8 эффективность снижается с 99.0% до 44.4%. Это связано с тем, что каждый дополнительный процесс увеличивает количество операций MPI_Allreduce для поиска главного элемента, чем больше процессов, тем выше вероятность, что переставляемые строки находятся на разных процессах, что требует дополнительных коммуникаций.
3. **Оптимальное количество процессов.** Максимальное абсолютное ускорение достигается на 8 процессах (3.55 раза), однако наилучшая эффективность использования ресурсов наблюдается на 2 процессах (99.0%). Для данной задачи оптимальным компромиссом между ускорением и эффективностью являются 4 процесса.
4. **Влияние ленточной структуры**: Ленточная структура матрицы снижает вычислительную сложность с O(n³) до O(n×m²), но также ограничивает параллелизм, так как каждый процесс работает только с ограниченным подмножеством строк. Ширина ленты m=100 составляет всего 1% от размера системы, это позволяет эффективно использовать кэш процессора. При увеличении размера системы пропорционально возрастает и вычислительная нагрузка, что может улучшить масштабируемость для большего количества процессов. Для систем размером 50,000+ уравнений можно ожидать лучшей эффективности на 6-8 процессах.

## 8. Заключение

Успешно реализованы последовательный и параллельный алгоритмы метода Гаусса для ленточных матриц с использованием горизонтальной схемы распределения данных. Особенностью реализации является эффективная работа с ленточным форматом хранения матриц, что позволяет значительно экономить память и вычислительные ресурсы при решении больших разреженных систем.
Параллельная реализация демонстрирует хорошее ускорение до 3.55 раза при использовании 8 процессов, что является значительным улучшением. Наибольшая эффективность использования ресурсов (99.0%) достигается при использовании 2 процессов, что делает эту конфигурацию оптимальной для тестовой системы.
Реализованный алгоритм эффективно решает СЛАУ с ленточными матрицами и может быть использован в различных расчетах, где требуются решения больших разреженных систем уравнений.

## 9. Список литературы
1. Chandra R. Parallel programming in OpenMP. – Morgan kaufmann, 2001.
2. R. L. Graham, G. M. Shipman, B. W. Barrett, R. H. Castain, G. Bosilca and A. Lumsdaine, "Open MPI: A High-Performance, Heterogeneous MPI," 2006 IEEE International Conference on Cluster Computing, Barcelona, Spain, 2006, pp. 1-9, doi: 10.1109/CLUSTR.2006.311904.
3. В.П. Гергель. Учебный курс "Введение в методы параллельного программирования". Раздел "Параллельное программирование с использованием OpenMP" // URL: http://www.hpcc.unn.ru/multicore/materials/tb/mc_ppr04.pdf, 2007.
4. Документация по курсу «Параллельное программирование» // URL: https://learning-process.github.io/parallel_programming_course/ru/index.html, 2025.