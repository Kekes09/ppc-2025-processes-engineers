# Сетевая топология Кольцо
  -Студент: Кондрашова Виктория Андреевна, group 3823Б1ПР1
  -Технологии: SEQ | MPI
  -Вариант: 7

## 1. Введение
Кольцевая топология является одной из фундаментальных сетевых структур в распределенных вычислительных системах. Она широко благодаря своей простоте, предсказуемости маршрутизации и отказоустойчивости. В рамках этого исследования проводится сравнительный анализ производительности последовательной и параллельной MPI-реализаций алгоритма передачи данных по кольцевой топологии.

## 2. Постановка задачи
**Цель работы:** Реализовать две версии алгоритма передачи данных по кольцевой топологии — последовательную (SEQ) и параллельную (MPI) с явной маршрутизацией, провести их сравнение и анализ эффективности.

**Определение задачи:** Для набора процессов, организованных в кольцевую топологию, необходимо передать вектор данных от процесса-отправителя (source) к процессу-получателю (recipient), используя последовательную передачу через промежуточные узлы кольца.

**Ограничения:**
 - Корректность передачи должна сохраняться при любых значениях source и recipient
 - Результаты SEQ и MPI версий должны полностью совпадать
 - Каждый процесс может взаимодействовать только с соседями (prev и next)
 - Входные данные: 
    - source — ранг процесса-отправителя
    - recipient — ранг процесса-получателя
    - data — вектор целых чисел произвольного размера
 - Выходные данные: вектор data, полученный на процессе recipient

## 3. Алгоритм(Последовательная версия)
**Шаги алгоритма:**
 - Разбор входных данных

 - Валидация (ValidationImpl): 
    - Проверка корректности входных данных

 - Предобработка (PreProcessingImpl): инициализация выходного вектора (очистка)

 - Основное вычисление (RunImpl):
    - Копирование данных в выходной вектор
 - Возврат: Выходной вектор, соответсвующий вектору, данному на вход

## 4. Схема распараллеливания
Этапы выполнения
### 1. Валидация (ValidationImpl)
Процесс 0:**

- Проверяет корректность входных данных

**Все процессы:**

- Получают результат валидации через MPI_Bcast
- Возвращают единый результат

### 2. Предобработка (PreProcessingImpl)

**Все процессы:**

- Инициализируют выходной вектор: GetOutput().clear()

### 3. Основное вычисление (RunImpl)
### 3.1. РРаспределение данных от root процесса
**Процесс 0:**

- Извлекает source, recipient, data_size из входных данных
- Подготавливает вектор data для передачи

**Все процессы:**

- Выделяют память: std::vector<int> data(data_size)
### 3.2. Обработка особых случаев
**Случай 1: source == recipient**

- Процесс recipient копирует данные в выходной вектор
- Остальные процессы ничего не делают
- Возврат true
**Случай 2: world_size == 1**

- Единственный процесс копирует данные в выход
- Возврат true

### 3.3. Вычисление маршрута передачи

### 3.4. Передача данных по кольцу

### 4. Постобработка (PostProcessingImpl)

## 5.Детали реализации
### Структура кода

**Файлы:**
- `common/include/common.hpp` - определение типов данных
- `seq/include/ops_seq.hpp`, `seq/src/ops_seq.cpp` - последовательная реализация
- `mpi/include/ops_mpi.hpp`, `mpi/src/ops_mpi.cpp` - параллельная реализация
- `tests/functional/main.cpp` - функциональные тесты
- `tests/performance/main.cpp` - тесты производительности

**Классы:**
- `KondrashovaVRingTopologySEQ` - последовательная версия (SEQ)
- `KondrashovaVRingTopologyMPI` - параллельная версия (MPI)

**Методы (одинаковы для обеих реализаций):**
- `ValidationImpl()` - проверка корректности входа
- `PreProcessingImpl()` - подготовка данных
- `RunImpl()` - основная логика (последовательная или MPI).
- `PostProcessingImpl()` - проверка корректности выхода.


## 6. Экспериментальная установка
- Hardware/OS: CPU - AMD Ryzen 5 5600H, 6 ядер/12 потоков; RAM - 16 Gb; ОС - Windows 10 
- Toolchain: g++ 11.4.0 , build type: Release  
- Environment: PPC_NUM_PROC
- Data: тестовые данные задаются вручную.

## 7. Результаты и обсуждения

### 7.1 Корректность

**Корректность проверена через:**

* Модульные тесты:
    - Передача между соседними процессами (source=0, recipient=1)
    - Передача в обратном направлении (через всё кольцо)
    - Передача процессу самому себе (source == recipient)

* Сравнение между последовательными и MPI результатами

* Все 12 функциональных тестов успешно пройдены

### 7.2 Производительность
**Характеристики задачи:**
- Размер: 100,000,000 элементов
- Выходной вектор: 100,000,000 элементов

Speedup = T_seq / T_parallel  
Efficiency = Speedup / Count * 100%

| Mode        | Count | Time, s | Speedup | Efficiency |
|-------------|-------|---------|---------|------------|
| seq         | 1     | 2.1660  | 1.00    | N/A        |
| mpi         | 4     | 3.3540  | 0.65    | 16.3%      |
| mpi         | 8     | 5.7480  | 0.38    | 4.75%      |

## 8. Вывод
В ходе выполнения работы была создана и протестирована программная реализация задачи передачи данных по кольцевой сетевой топологии. Разработка включала два подхода: последовательную версию (SEQ) и параллельную версию (MPI) с явной маршрутизацией и ручным управлением соседями в кольце.

Разработанные алгоритмы корректно решают поставленную задачу, что подтверждается успешным прохождением всех 12 функциональных тестов, включая граничные случаи (пустые данные, передача самому себе, большие объёмы данных, отрицательные числа) и проверкой идентичности результатов между SEQ и MPI версиями. Из полученных результатов видно, что MPI-версия работает медленнее Seq-версии, это связано с высокими накладными расходами на межпроцессное взаимодействие.

## 9. Источники
1. Список лекций по курсу "Параллельное программирование". (Сысоев А.В. ННГУ 2025 г.)
2. Документация по MPI: https://learn.microsoft.com/ru-ru/message-passing-interface/mpi-process-topology-functions
2. Список практических занятий по курсу "Пареллельное программирование". (Оболенский А.А, ННГУ 2025 г.)
3. Документация по курсу: "Параллельное программирование": <https://learning-process.github.io/parallel_programming_course/ru/index.html> (Оболенский А.А, Нестеров А.Ю)

## 10. Приложение

### MPI-реализация(ключевой алгоритм)
```cpp
bool KondrashovaVRingTopologyMPI::RunImpl() {
  int rank = 0;
  int world_size = 0;
  MPI_Comm_rank(MPI_COMM_WORLD, &rank);
  MPI_Comm_size(MPI_COMM_WORLD, &world_size);

  int source = 0;
  int recipient = 0;
  int data_size = 0;

  if (rank == 0) {
    source = GetInput().source;
    recipient = GetInput().recipient;
    data_size = static_cast<int>(GetInput().data.size());
  }

  MPI_Bcast(&source, 1, MPI_INT, 0, MPI_COMM_WORLD);
  MPI_Bcast(&recipient, 1, MPI_INT, 0, MPI_COMM_WORLD);
  MPI_Bcast(&data_size, 1, MPI_INT, 0, MPI_COMM_WORLD);

  std::vector<int> data(data_size);
  if (rank == 0) {
    data = GetInput().data;
  }
  if (data_size > 0) {
    MPI_Bcast(data.data(), data_size, MPI_INT, 0, MPI_COMM_WORLD);
  }

  if (world_size == 1 || source == recipient) {
    if (rank == recipient) {
      GetOutput() = data;
    }
    return true;
  }

  int steps = (recipient - source + world_size) % world_size;

  std::vector<int> buffer;

  for (int step = 0; step < steps; step++) {
    int sender = (source + step) % world_size;
    int receiver = (sender + 1) % world_size;

    SendData(rank, sender, receiver, step, data_size, data, buffer);
    ReceiveData(rank, sender, receiver, recipient, buffer);

    MPI_Barrier(MPI_COMM_WORLD);
  }

  return true;
}
```
