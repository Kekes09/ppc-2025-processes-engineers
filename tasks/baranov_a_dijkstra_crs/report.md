# Отчёт: Реализация алгоритма Дейкстры для разреженных графов (CRS формат) с использованием MPI

**Студент:** Баранов Андрей Александрович  
**Группа:** 3823Б1ПР4  
**Технология:** MPI  
**Задача:** Параллельная реализация алгоритма Дейкстры для нахождения кратчайших путей в разреженном графе


## 1. Вступление

В задачах поиска кратчайших путей в графах алгоритм Дейкстры является одним из фундаментальных.
Для больших разреженных графов эффективное использование памяти становится критически важным, что приводит к использованию форматов хранения графов, таких как CRS (Compressed Row Storage).
В данной работе реализована параллельная версия алгоритма Дейкстры с использованием MPI для распределения вычислений между несколькими процессами. Основной целью было ускорить обработку больших графов за счёт параллельного обхода вершин и распределённого хранения данных о графе.


## 2. Задача

### 2.1. Формальное определение

Требуется реализовать алгоритм Дейкстры для нахождения кратчайших путей от заданной исходной вершины до всех остальных вершин в графе. Граф представлен в формате CRS, что особенно эффективно для разреженных графов. Реализация должна включать как последовательную (SEQ), так и параллельную (MPI) версии для сравнения производительности.

### 2.2. Входные и выходные данные

**Входные данные:**
- `num_vertices` – количество вершин в графе
- `source_vertex` – исходная вершина (от которой ищутся расстояния)
- `offsets` – массив смещений для формата CRS (длины `num_vertices + 1`)
- `columns` – массив номеров смежных вершин
- `values` – массив весов рёбер

**Выходные данные:**
- `distances` – массив кратчайших расстояний от исходной вершины до всех вершин графа

### 2.3. Формат CRS

CRS (Compressed Row Storage) формат эффективно хранит разреженные матрицы:
- `offsets[i]` указывает начальный индекс в массивах `columns` и `values` для рёбер, исходящих из вершины `i`
- `offsets[i+1] - offsets[i]` равно количеству рёбер, исходящих из вершины `i`
- `columns` содержит номера смежных вершин
- `values` содержит веса соответствующих рёбер


## 3. Последовательная реализация (SEQ)

В последовательной реализации используется стандартный алгоритм Дейкстры с приоритетной очередью:

```
bool BaranovADijkstraCRSSEQ::RunImpl() {
  const auto &graph = GetInput();
  const int n = graph.num_vertices;
  const int source = graph.source_vertex;
  std::vector<double> dist(n, std::numeric_limits<double>::infinity());
  dist[source] = 0.0;

  std::priority_queue<std::pair<double, int>, 
                      std::vector<std::pair<double, int>>, 
                      std::greater<>> pq;
  pq.emplace(0.0, source);

  std::vector<bool> visited(n, false);

  while (!pq.empty()) {
    auto [current_dist, u] = pq.top();
    pq.pop();

    if (visited[u]) continue;
    visited[u] = true;

    int start = graph.offsets[u];
    int end = graph.offsets[u + 1];

    for (int idx = start; idx < end; ++idx) {
      int v = graph.columns[idx];
      double weight = graph.values[idx];

      if (!visited[v]) {
        double new_dist = current_dist + weight;
        if (new_dist < dist[v]) {
          dist[v] = new_dist;
          pq.emplace(new_dist, v);
        }
      }
    }
  }

  GetOutput() = dist;
  return true;
}
```

# 4. Параллельная реализация (MPI)

## 4.1. Основная идея
Параллельная реализация использует распределение вершин графа между процессами. Каждый процесс отвечает за обработку определённого подмножества вершин и их рёбер. Для синхронизации расстояний используется операция `MPI_Allreduce` с операцией `MPI_MIN`.

## 4.2. Распределение данных
Вершины графа распределяются между процессами с учётом остатка:

```
void CalculateVertexDistribution(int world_rank, int world_size, int total_vertices, 
                                 int &local_start, int &local_end, int &local_count) {
  int vertices_per_process = total_vertices / world_size;
  int remainder = total_vertices % world_size;

  if (world_rank < remainder) {
    local_start = world_rank * (vertices_per_process + 1);
    local_end = local_start + vertices_per_process + 1;
  } else {
    local_start = (remainder * (vertices_per_process + 1)) + 
                  ((world_rank - remainder) * vertices_per_process);
    local_end = local_start + vertices_per_process;
  }
  local_count = local_end - local_start;
}
```

## 4.3. Алгоритм параллельного выполнения

Основной алгоритм в функции `RunImpl()`:

1. **Распределение данных графа:**
   - Каждый процесс получает свою порцию вершин
   - Сохраняются локальные смещения, столбцы и значения

2. **Инициализация расстояний:**
   - Глобальный массив расстояний инициализируется бесконечностью
   - Расстояние до исходной вершины устанавливается в 0
   - Если исходная вершина принадлежит процессу, он становится её "владельцем"

3. **Итеративный процесс обновления:**
   - Каждый процесс обрабатывает свои локальные вершины
   - Для каждой вершины обновляются расстояния до смежных вершин
   - Используется `MPI_Allreduce` с `MPI_MIN` для глобальной синхронизации расстояний
   - Процесс повторяется до тех пор, пока происходят изменения

## 4.4. Ключевые функции

**Обработка соседей вершины:**

```
bool ProcessNeighbors(const std::vector<double> &local_dist, int global_v, 
                      const std::vector<int> &local_offsets, int i,
                      const std::vector<int> &local_columns, 
                      const std::vector<double> &local_values,
                      int total_vertices, std::vector<double> &new_dist) {
  bool changed = false;
  int start = local_offsets[i];
  int end = local_offsets[i + 1];

  for (int idx = start; idx < end; ++idx) {
    int neighbor = local_columns[idx];
    double weight = local_values[idx];
    double new_distance = local_dist[global_v] + weight;
    
    if (new_distance < new_dist[neighbor]) {
      new_dist[neighbor] = new_distance;
      changed = true;
    }
  }
  return changed;
}
```

**Обработка соседей вершины:**

```
bool UpdateDistances(std::vector<double> &global_dist, 
                     std::vector<double> &local_dist, 
                     std::vector<double> &new_dist,
                     int total_vertices) {
  MPI_Allreduce(new_dist.data(), global_dist.data(), 
                total_vertices, MPI_DOUBLE, MPI_MIN, MPI_COMM_WORLD);
  
  local_dist.assign(global_dist.begin(), global_dist.end());
  new_dist.assign(global_dist.begin(), global_dist.end());
  
  return true;
}
```

## 5. Схема коммуникации

```
┌─────────┐     ┌─────────┐     ┌─────────┐
│Процесс 0│     │Процесс 1│     │Процесс 2│
│Вершины  │     │Вершины  │     │Вершины  │
│ 0..k-1  │     │ k..m-1  │     │ m..n-1  │
└────┬────┘     └────┬────┘     └────┬────┘
     │               │               │
     │  Обработка    │  Обработка    │  Обработка
     │  локальных    │  локальных    │  локальных
     │   вершин      │   вершин      │   вершин
     │               │               │
     └───────┬───────┴───────┬───────┘
             │ MPI_Allreduce │
             │   (MPI_MIN)   │
             └───────┬───────┘
                     │
             Глобальные расстояния
```

## 6. Детали реализации

### 6.1. Файловая структура
baranov_a_dijkstra_crs/
├── common/include/common.hpp
├── mpi/include/ops_mpi.hpp
├── mpi/src/ops_mpi.cpp
├── seq/include/ops_seq.hpp
├── seq/src/ops_seq.cpp
├── tests/functional/main.cpp
├── tests/performance/main.cpp
└── CMakeLists.txt


### 6.2. Ключевые классы

- **BaranovADijkstraCRSSEQ** – последовательная реализация:
  - Использует приоритетную очередь для эффективного выбора следующей вершины
  - Временная сложность: O((V+E) log V)

- **BaranovADijkstraCRSMPI** – параллельная реализация:
  - Распределяет вершины между процессами
  - Использует итеративный подход с глобальной синхронизацией
  - Временная сложность: O(V*(V/p + E/p)) с накладными расходами на коммуникацию


## 7. Конфигурация системы и инструменты

### 7.1. Системные характеристики

- **Модель процессора:** AMD Ryzen 5 5600x
- **Количество ядер:** 6 (12 потоков)
- **Тактовая частота:** 3.7 GHz
- **Архитектура:** x64
- **Оперативная память:** 32 GB DDR4 3200 MHz
- **Операционная система:** Windows 10 PRO 64-bit

### 7.2. Набор инструментов

- **Компилятор:** MSVC 2022 (V19.32)
- **Стандарт языка:** C++17
- **Среда разработки:** Visual Studio 2022
- **Система сборки:** CMake


## 8. Результаты

### 8.1. Функциональное тестирование

Все 28 тестов (14 для MPI и 14 для SEQ) успешно пройдены для всех конфигураций (1, 2, 3, 4 процесса). Тесты покрывают различные сценарии:

1. `simple_graph_4_vertices` – простой граф из 4 вершин
2. `no_paths_graph` – граф без путей
3. `linear_graph` – линейный граф
4. `complete_graph_3_vertices` – полный граф из 3 вершин
5. `single_node_graph` – граф из одной вершины
6. `bidirectional_graph` – двунаправленный граф
7. `graph_with_multiple_paths` – граф с несколькими путями
8. `star_graph` – звездообразный граф
9. `cyclic_graph` – циклический граф
10. `multiple_paths_to_target` – несколько путей к цели
11. `non_zero_source` – исходная вершина не 0
12. `large_sparse_graph` – большой разреженный граф
13. `graph_with_self_loops` – граф с петлями
14. `fully_connected_graph` – полностью связный граф

### 8.2. Производительность

Результаты для графа с 1000 вершинами и ~10 рёбрами на вершину:

| Процессы | Режим    | Время MPI (с) | Время SEQ (с) | Ускорение |
|----------|----------|---------------|---------------|-----------|
| 1        | pipeline | 0.00153942    | 0.00357876    | 2.32x     |
| 1        | task_run | 0.00118834    | 0.00347178    | 2.92x     |
| 2        | pipeline | 0.00092798    | 0.00359816    | 3.88x     |
| 2        | task_run | 0.00095436    | 0.00357072    | 3.74x     |
| 3        | pipeline | 0.00075436    | 0.00378110    | 5.01x     |
| 3        | task_run | 0.00086688    | 0.00368830    | 4.25x     |
| 4        | pipeline | 0.00068388    | 0.00356454    | 5.21x     |
| 4        | task_run | 0.00064444    | 0.00361852    | 5.62x     |

### 8.3. Анализ производительности

- **Эффективность параллелизации:**
  - При 2 процессах: ускорение ~3.8x (эффективность ~190%)
  - При 3 процессах: ускорение ~4.6x (эффективность ~153%)
  - При 4 процессах: ускорение ~5.4x (эффективность ~135%)

- **Превышение 100% эффективности:**
  - Сверхлинейное ускорение объясняется лучшим использованием кэшей процессора при работе с меньшими порциями данных
  - MPI версия использует более простой алгоритм без приоритетной очереди

- **Коммуникационные затраты:**
  - Использование `MPI_Allreduce` на каждой итерации создает значительные накладные расходы
  - Однако для данного размера задачи эти затраты компенсируются параллельной обработкой


## 9. Выводы

### 9.1. Достижения

- **Корректность реализации:**
  - Обе версии (SEQ и MPI) корректно вычисляют кратчайшие пути
  - Все функциональные тесты пройдены успешно
  - Поддерживаются различные типы графов

- **Эффективность:**
  - MPI реализация демонстрирует сверхлинейное ускорение
  - Эффективное использование формата CRS для разреженных графов
  - Хорошая масштабируемость до 4 процессов

- **Архитектурные решения:**
  - Чистое разделение SEQ и MPI реализаций
  - Модульная структура кода
  - Полное покрытие тестами

### 9.2. Ограничения

- **Алгоритмические:**
  - MPI версия использует итеративный подход вместо приоритетной очереди
  - В худшем случае требует V итераций
  - Не оптимальна для плотных графов

- **Коммуникационные:**
  - `MPI_Allreduce` вызывается на каждой итерации
  - Пересылается весь массив расстояний размером V
  - Для очень больших графов это может стать узким местом

- **Распределение данных:**
  - Статическое распределение вершин без учёта степени вершин
  - Несбалансированная нагрузка возможна для графов с неравномерным распределением рёбер

### 9.3. Рекомендации по улучшению

- **Динамическая балансировка нагрузки:**
  - Распределение вершин с учётом количества исходящих рёбер
  - Возможность перераспределения вершин во время выполнения

- **Оптимизация коммуникаций:**
  - Использование асинхронных операций MPI
  - Локализация обновлений расстояний
  - Пакетная обработка обновлений

- **Гибридный подход:**
  - Комбинация MPI и OpenMP для использования многопоточности внутри узлов
  - Более эффективное использование современных многоядерных процессоров

### 9.4. Заключение

Реализация успешно демонстрирует преимущества параллельного подхода к решению задачи поиска кратчайших путей в разреженных графах. 
MPI версия показывает значительное ускорение по сравнению с последовательной реализацией, особенно при увеличении числа процессов. 
Несмотря на некоторые ограничения алгоритмического характера, реализация корректна, эффективна и хорошо масштабируется в рамках рассмотренных тестовых конфигураций.
Работа может служить основой для более оптимизированных реализаций, учитывающих специфику различных типов графов и архитектур вычислительных систем.
