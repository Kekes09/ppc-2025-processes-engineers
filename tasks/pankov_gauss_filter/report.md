# Линейная фильтрация изображений (вертикальное разбиение). Ядро Гаусса 3×3

- Студент: Паньков Андрей Владимирович, группа 3823Б1ПР3
- Технология: MPI + SEQ
- Задача: 3 (вариант 27)

## 1. Введение

Линейная фильтрация изображений — базовая операция компьютерного зрения и обработки сигналов. Одним из наиболее часто используемых фильтров сглаживания является гауссов фильтр, который уменьшает шум и подавляет высокочастотные компоненты.

Цель работы — реализовать последовательную (SEQ) и параллельную (MPI) версии фильтрации изображения **гауссовым ядром 3×3**, используя **вертикальную схему разбиения** изображения (по столбцам), а также оценить производительность и эффективность распараллеливания.

## 2. Постановка задачи

**Входные данные:** изображение размером \(W \times H\) с числом каналов \(C\), представленное **одномерным массивом** `uint8_t` в формате row-major:

`data[(y * width + x) * channels + c]`

**Выходные данные:** изображение того же размера и числа каналов, полученное после свёртки гауссовым ядром 3×3.

**Математическая модель:**

\[
K = \frac{1}{16}
\begin{pmatrix}
1 & 2 & 1\\
2 & 4 & 2\\
1 & 2 & 1
\end{pmatrix}
\]

Для каждого пикселя \((x, y)\) и канала \(c\):

\[
out(x, y, c) = \sum_{dy=-1}^{1}\sum_{dx=-1}^{1} K(dx, dy)\cdot in(x+dx, y+dy, c)
\]

## 3. Базовый алгоритм (последовательный)

Последовательный алгоритм выполняет свёртку по окрестности 3×3 для каждого пикселя и канала.

**Обработка границ:** используется **clamp-to-edge** (координаты за пределами изображения прижимаются к границе).

**Округление и диапазон:** вычисление ведётся в `int`, затем применяется \((sum + 8) / 16\) и ограничение результата в \([0, 255]\).

**Сложность:**

- Временная: \(O(W \cdot H \cdot C)\)
- Пространственная: \(O(W \cdot H \cdot C)\) для выходного изображения

## 4. Схема параллелизации

### 4.1 Распределение данных (вертикальная схема)

Изображение делится на вертикальные полосы (по столбцам):

- `base_cols = width / size`
- `rem_cols = width % size`
- процесс `rank < rem_cols` получает на 1 столбец больше

Каждый процесс получает свою полосу ширины `local_w`.

### 4.2 Вычисление частичных результатов

Каждый процесс выполняет свёртку только для своих столбцов. Для корректной свёртки 3×3 по оси X нужны соседние столбцы (halo).

### 4.3 Коммуникационная схема

**Операции MPI:**

1. Передача полос изображения с процесса 0 на остальные процессы (вертикальные полосы) через `MPI_Send`/`MPI_Recv`.
   Так как полоса по столбцам не является непрерывным куском исходного row-major массива, на процессе 0 полоса
   предварительно **упаковывается** в непрерывный буфер (row-major внутри полосы).
2. `MPI_Sendrecv` — обмен halo-столбцами между соседними процессами
3. `MPI_Gatherv` — сбор локальных результатов на процессе 0
4. `MPI_Bcast` — рассылка итогового изображения всем процессам (для одинакового `output` на всех рангах)

### 4.4 Роли процессов

- **Процесс 0:** распределяет полосы, собирает результат и рассылает итоговое изображение
- **Процессы 1..n-1:** получают полосу, обмениваются halo, вычисляют локальную часть, участвуют в сборе

## 5. Детали реализации

### 5.1 Структура кода

Код организован следующим образом:

- `common/include/common.hpp` — типы `Image`, `InType`, `OutType`
- `seq/src/ops_seq.cpp` — последовательная реализация (`PankovGaussFilterSEQ`)
- `mpi/src/ops_mpi.cpp` — MPI-реализация (`PankovGaussFilterMPI`)
- `tests/functional/main.cpp` — функциональные тесты
- `tests/performance/main.cpp` — тесты производительности

### 5.2 Классы задач

Обе версии наследуют `ppc::task::Task<InType, OutType>` и реализуют этапы:

- `ValidationImpl()`
- `PreProcessingImpl()`
- `RunImpl()`
- `PostProcessingImpl()`

### 5.3 Распределение данных (полосы по столбцам)

Изображение логически делится на полосы по столбцам. На практике данные для каждой полосы упаковываются в непрерывный
буфер (row-major внутри полосы) и передаются соответствующему процессу через `MPI_Send`/`MPI_Recv`.

### 5.4 Вычисление и агрегация результатов

- Локальная свёртка: каждый процесс считает пиксели своей полосы
- Halo-обмен: `MPI_Sendrecv` со смежными процессами (левый/правый столбец)
- Сбор: `MPI_Gatherv` на rank0 и `MPI_Bcast` итогового изображения

### 5.5 Использование памяти

- SEQ: хранит вход и выход целиком — \(O(W \cdot H \cdot C)\)
- MPI:
  - процесс 0: хранит вход и итоговый выход целиком (для сборки)
  - остальные процессы: вход/выход только своей полосы + halo столбцы

## 6. Настройка эксперимента

### 6.1 Аппаратное окружение

- **CPU:** Intel(R) Xeon(R) CPU E5-2680 v4 @ 2.40GHz
- **Количество CPU:** 14
- **ОЗУ:** 32 GiB 
- **ОС:** Linux (WSL2)

### 6.2 Программное окружение

- **Компилятор:** g++ 13.3.0
- **MPI:** Open MPI 4.1.6
- **Система сборки:** CMake
- **Фреймворк тестирования:** Google Test

### 6.3 Параметры окружения

- `PPC_NUM_PROC` — количество MPI-процессов
- `PPC_NUM_THREADS` — количество потоков (для этой задачи не используется, но требуется окружением)
- `OMPI_ALLOW_RUN_AS_ROOT=1`, `OMPI_ALLOW_RUN_AS_ROOT_CONFIRM=1` — разрешение запуска OpenMPI от root (контейнер)

### 6.4 Тестовые данные

- **Функциональные тесты:** небольшие синтетические изображения (grayscale и RGB)
- **Тесты производительности:** изображение 4096×4096, 1 канал (grayscale), заполнение
  `img[row, col] = (row + col) % 256`

### 6.5 Команды запуска

**Performance (пример):**

```bash
export OMPI_ALLOW_RUN_AS_ROOT=1
export OMPI_ALLOW_RUN_AS_ROOT_CONFIRM=1
export PPC_NUM_THREADS=1
mpirun --allow-run-as-root --oversubscribe -np 4 ./build/bin/ppc_perf_tests --gtest_filter='*pankov_gauss_filter_mpi_enabled*'
./build/bin/ppc_perf_tests --gtest_filter='*pankov_gauss_filter_seq_enabled*'
```

## 7. Результаты и обсуждение

### 7.1 Корректность

Корректность проверяется сравнением результата SEQ/MPI с эталонной реализацией свёртки на небольших синтетических изображениях (grayscale и RGB). Все тесты пройдены.

### 7.2 Производительность

Ниже приведены результаты (использовалось время **pipeline**). Базовое время SEQ: `T_seq = 0.1625783920` с.

| Режим | Процессы | Время, с       | Ускорение | Эффективность |
|-------|----------|----------------|-----------|---------------|
| seq   | 1        | 0.1625783920   | 1.00      | N/A           |
| mpi   | 1        | 0.2850076972   | 0.57      | 57.0%         |
| mpi   | 2        | 0.2254145984   | 0.72      | 36.1%         |
| mpi   | 4        | 0.1965714584   | 0.83      | 20.7%         |
| mpi   | 8        | 0.2093870330   | 0.78      | 9.7%          |

**Метрики:**

- **Ускорение (Speedup):** \(S(p) = T_{seq} / T_{mpi}(p)\)
- **Эффективность (Efficiency):** \(E(p) = S(p) / p \times 100\%\)

### 7.3 Анализ производительности

Для 4096×4096 вычислительная нагрузка значительно выше, но при увеличении числа процессов эффективность всё равно ограничивается накладными расходами MPI (распределение данных, halo-обмен и синхронизация). Лучшее время наблюдается около 4 процессов; при 8 процессах накладные расходы начинают доминировать над выигрышем от параллелизма.

## 8. Выводы

Реализованы последовательная и MPI-версии линейной фильтрации изображения гауссовым ядром 3×3. MPI-версия использует **вертикальное разбиение** и обмен halo-столбцами для корректной свёртки. Эксперименты показали, что ускорение ограничено коммуникационными накладными расходами; оптимум для данного стенда находится в районе 4 процессов.

## 9. Список литературы

1. Документация MPI: `https://www.mpi-forum.org/docs/`
2. Open MPI Documentation: `https://www.open-mpi.org/doc/`

## Приложение

### Гауссово ядро 3×3

Используемое ядро:

\[
\frac{1}{16}
\begin{pmatrix}
1 & 2 & 1\\
2 & 4 & 2\\
1 & 2 & 1
\end{pmatrix}
\]
