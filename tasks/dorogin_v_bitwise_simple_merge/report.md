# Поразрядная сортировка для вещественных чисел с простым слиянием

- Студент: Дорогин Вадим Антонович, group 3823Б1ПР3
- Технология: MPI|SEW
- Вариант: 20

## 1. Введение

Сортировка больших массивов данных является одной из фундаментальных задач в вычислительной технике. Для эффективной обработки больших объемов данных необходимо использовать параллельные алгоритмы, которые позволяют распределить вычислительную нагрузку между несколькими процессами или потоками.

Поразрядная сортировка (radix sort) является эффективным алгоритмом сортировки, который работает за линейное время O(n) для чисел фиксированной разрядности. Однако для вещественных чисел типа double требуется специальная обработка из-за особенностей представления чисел с плавающей точкой в памяти.

Целью данной работы является реализация параллельной поразрядной сортировки для массивов вещественных чисел типа double с использованием технологии MPI и простого слияния отсортированных частей.

## 2. Постановка задачи

Требуется реализовать алгоритм поразрядной сортировки для массивов вещественных чисел типа double с использованием простого слияния.

Входные данные: массив вещественных чисел типа double произвольного размера.

Выходные данные: отсортированный по возрастанию массив тех же чисел.

Ограничения:
- Необходимо реализовать последовательную и параллельную версии алгоритма
- Параллельная версия должна использовать технологию MPI
- В методе RunImpl должна использоваться функция MPI_Send или MPI_Scatterv
- Алгоритм должен корректно обрабатывать положительные и отрицательные числа
- Алгоритм должен корректно обрабатывать граничные случаи (пустой массив, один элемент)

## 3. Описание последовательного алгоритма 

Последовательный алгоритм поразрядной сортировки представляет собой следующее:

### 3.1 Преобразование чисел в сортируемые ключи

Вещественные числа типа double представлены в формате IEEE 754, где старший бит является знаковым. Для корректной сортировки необходимо преобразовать числа в ключи, которые можно сортировать как целые числа без знака.

Алгоритм преобразования:
1. Скопировать битовое представление числа в 64-битное целое число без знака
2. Если число отрицательное (старший бит равен 1), инвертировать все биты
3. Если число положительное (старший бит равен 0), инвертировать только старший бит

Это преобразование обеспечивает корректный порядок сортировки: отрицательные числа будут меньше положительных, а внутри каждой группы порядок будет правильным.

### 3.2 Поразрядная сортировка

Алгоритм поразрядной сортировки работает следующим образом:

1. Преобразовать все числа в сортируемые ключи
2. Выполнить сортировку по разрядам, начиная с младших:
   - Для каждого байта (8 бит) ключа:
     - Подсчитать количество элементов в каждой из 256 корзин
     - Вычислить префиксные суммы для определения позиций элементов
     - Переместить элементы в новую последовательность согласно их байту
3. Преобразовать отсортированные ключи обратно в вещественные числа

Алгоритм выполняет 8 проходов, каждый проход имеет сложность O(n).

## 4. Схема распараллеливания

### 4.1 Распределение данных

Процесс с рангом 0 распределяет исходный массив между всеми процессами:
- Вычисляется размер части для каждого процесса: base_chunk = total_size / num_processes
- Остаток распределяется между первыми процессами: remainder = total_size % num_processes
- Каждый процесс получает base_chunk + (1 если его ранг < remainder, иначе 0) элементов

### 4.2 Передача данных

Главный процесс отправляет части массива остальным процессам с помощью MPI_Send:
- Для каждого процесса вычисляется смещение в исходном массиве
- Главный процесс отправляет соответствующую часть каждому процессу
- Каждый процесс получает свою часть с помощью MPI_Recv

### 4.3 Локальная сортировка

Каждый процесс выполняет поразрядную сортировку своей части массива независимо от других процессов.

### 4.4 Слияние результатов

После локальной сортировки процессы отправляют отсортированные части обратно главному процессу:
- Каждый процесс (кроме главного) отправляет свою отсортированную часть главному процессу
- Главный процесс последовательно сливает полученные части с помощью простого слияния:
  - Сравниваются первые элементы двух отсортированных массивов
  - Меньший элемент добавляется в результирующий массив
  - Процесс повторяется до исчерпания одного из массивов
  - Остатки второго массива добавляются в конец результата

### 4.5 Распространение результата

Главный процесс распространяет итоговый отсортированный массив всем процессам с помощью MPI_Bcast для обеспечения согласованности данных.

## 5. Детали реализации

### 5.1 Структура кода

Реализация состоит из следующих компонентов:

- common/include/common.hpp - общие определения типов (InType = std::vector<double>, OutType = std::vector<double>)
- seq/include/ops_seq.hpp и seq/src/ops_seq.cpp - последовательная реализация
- mpi/include/ops_mpi.hpp и mpi/src/ops_mpi.cpp - параллельная реализация с MPI
- tests/functional/main.cpp - функциональные тесты
- tests/performance/main.cpp - тесты производительности

### 5.2 Ключевые функции

В последовательной версии:
- DoubleToSortableKey - преобразование double в сортируемый ключ
- SortableKeyToDouble - обратное преобразование
- RadixSort - основная функция поразрядной сортировки

В параллельной версии дополнительно:
- SimpleMerge - функция простого слияния двух отсортированных массивов
- DistributeData - распределение данных между процессами
- GatherAndMerge - сбор и слияние результатов

### 5.3 Обработка граничных случаев

- Пустой массив: возвращается пустой результат без выполнения сортировки
- Массив из одного элемента: возвращается без изменений
- Массив, где все элементы одинаковы: корректно обрабатывается алгоритмом

### 5.4 Использование памяти

Алгоритм требует дополнительной памяти:
- O(n) для хранения ключей при сортировке
- O(n) для временного массива при каждом проходе сортировки
- O(n) для хранения результата слияния

Общая сложность по памяти: O(n)

## 6. Экспериментальная установка

### 6.1 Аппаратное обеспечение и ОС

- Процессор: 12th Gen Intel(R) Core(TM) i7-1260U (1.10 GHz)
- Количество ядер: 10
- Оперативная память: 16
- Операционная система: 10

### 6.2 Инструментарий

- Компилятор: MSVC
- Тип сборки: Release
- Версия CMake: 4.2.0

### 6.3 Тестовые данные

Тестовые данные генерируются программно:
- Размер массива: 1500000 элементов
- Диапазон значений: от -10000.0 до 10000.0
- Генератор случайных чисел: std::mt19937 с std::random_device в качестве seed

## 7. Результаты и обсуждение

### 7.1 Корректность

Корректность алгоритма проверялась следующими способами:

1. Функциональные тесты покрывают различные сценарии:
   - Пустой массив
   - Массив из одного элемента
   - Уже отсортированный массив
   - Обратно отсортированный массив
   - Массив с отрицательными числами
   - Массив со смешанными положительными и отрицательными числами
   - Массив с дублирующимися значениями
   - Массив, где все элементы одинаковы
   - Массивы с очень маленькими и очень большими значениями

2. Проверка результата:
   - Проверяется, что размер выходного массива равен размеру входного
   - Проверяется, что выходной массив отсортирован (используется std::ranges::is_sorted)
   - Для детерминированных тестов проверяется точное соответствие ожидаемому результату

3. Тесты производительности:
   - Проверяется корректность результата для больших массивов
   - Проверяется, что результат отсортирован

### 7.2 Производительность

Для оценки производительности использовались два режима тестирования:
- task_run: измеряется время выполнения только метода RunImpl
- pipeline: измеряется время выполнения всего пайплайна (Validation, PreProcessing, Run, PostProcessing)

Формулы для вычисления метрик:

Ускорение: Speedup = T_seq / T_par,
где T_seq - время выполнения последовательной версии, T_par - время выполнения параллельной версии.

Эффективность: Efficiency = Speedup / P * 100%, P - это количество процессов.

#### Таблица 1: Результаты для режима task_run

| Режим     | Процессов | Время, с | Ускорение | Эффективность |
|-----------|-----------|----------|-----------|---------------|
| seq       | 1         | 0.3378   | 1.00      | N/A           |
| mpi       | 1         | 0.2631   | 1.28      | 128.4%        |
| mpi       | 2         | 0.3429   | 0.98      | 49.2%         |
| mpi       | 4         | 0.3983   | 0.85      | 21.2%         |
| mpi       | 6         | 0.5738   | 0.59      | 9.8%          |
| mpi       | 8         | 0.6778   | 0.50      | 6.2%          |

#### Таблица 2: Результаты для режима pipeline

| Режим     | Процессов | Время, с | Ускорение | Эффективность |
|-----------|-----------|----------|-----------|---------------|
| seq       | 1         | 0.3850   | 1.00      | N/A           |
| mpi       | 1         | 0.3316   | 1.16      | 116.1%        |
| mpi       | 2         | 0.3064   | 1.26      | 62.8%         |
| mpi       | 4         | 0.3838   | 1.00      | 25.1%         |
| mpi       | 6         | 0.5670   | 0.68      | 11.3%         |
| mpi       | 8         | 0.6552   | 0.59      | 7.3%          |

#### Анализ результатов

Эксперименты проводились на одной локальной машине, что существенно влияет на результаты:

1. При запуске MPI на одном процессе (n=1) наблюдается небольшое ускорение по сравнению с последовательной версией. Это объясняется различиями в реализации и особенностями измерения времени.

2. При увеличении числа процессов (n=2 и более) производительность снижается. Это обусловлено следующими факторами:
   - Накладные расходы на межпроцессное взаимодействие MPI
   - Конкуренция процессов за ресурсы одной машины (CPU, память, кэш)
   - Последовательное слияние результатов на главном процессе

3. Простое слияние выполняется последовательно на процессе с рангом 0, что создает узкое место при увеличении числа процессов.

4. Для достижения реального ускорения необходимо запускать алгоритм на распределенной системе с несколькими физическими узлами, где накладные расходы на коммуникацию компенсируются параллельной обработкой на независимых вычислительных узлах.

Примечание: входные данные генерируются случайным образом для каждого запуска теста, поэтому абсолютные значения времени могут варьироваться между запусками

## 8. Выводы

В ходе выполнения работы была реализована параллельная поразрядная сортировка для вещественных чисел типа double с использованием технологии MPI и простого слияния:

- Реализован корректный алгоритм преобразования вещественных чисел в сортируемые ключи
- Реализована эффективная поразрядная сортировка с линейной сложностью
- Реализовано параллельное распределение данных с использованием MPI_Send
- Реализовано простое слияние отсортированных частей
- Обеспечена корректная обработка всех граничных случаев
- Достигнуто покрытие тестами более 90%

## 9. Источники

1. IEEE 754 Standard for Floating-Point Arithmetic. IEEE Computer Society, 2008.
2. OpenMPI Documentation. https://www.open-mpi.org/doc/
3. MPI: A Message-Passing Interface Standard. Version 3.1. Message Passing Interface Forum, 2015.
4. Сысоев А.В. Лекции по параллельному программированию, 2025
5. Radix Sort Algorithm. GeeksforGeeks. https://www.geeksforgeeks.org/radix-sort/

## Приложение

### Код преобразования чисел в сортируемые ключи

```cpp
uint64_t DoubleToSortableKey(double value) {
  uint64_t bits = 0;
  std::memcpy(&bits, &value, sizeof(double));
  if ((bits >> 63) != 0) {
    bits = ~bits;
  } else {
    bits ^= (static_cast<uint64_t>(1) << 63);
  }
  return bits;
}
```

### Код простого слияния

```cpp
void SimpleMerge(const std::vector<double>& left, 
                 const std::vector<double>& right, 
                 std::vector<double>& result) {
  result.clear();
  result.reserve(left.size() + right.size());
  size_t i = 0;
  size_t j = 0;
  while (i < left.size() && j < right.size()) {
    if (left[i] <= right[j]) {
      result.push_back(left[i++]);
    } else {
      result.push_back(right[j++]);
    }
  }
  while (i < left.size()) {
    result.push_back(left[i++]);
  }
  while (j < right.size()) {
    result.push_back(right[j++]);
  }
}
```

