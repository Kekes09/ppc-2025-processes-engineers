# Метод простой итерации

- **Студент:** Иванова Полина, группа 3823Б1ПР1  
- **Технология:** MPI, SEQ
- **Вариант:** 20
- **Преподаватель:** Сысоев Александр Владимирович, лектор, доцент кафедры высокопроизводительных вычислений и системного программирования

## 1. Введение

Целью данной работы является реализация параллельного варианта метода простой итерации для решения системы линейных уравнений с использованием технологии MPI (Message Passing Interface) и сравнение его производительности с последовательной (SEQ) версией.

## 2. Постановка задачи

**Задача:** Решить систему линейных уравнений **Ax = b** методом простой итерации, где:

- **A** — квадратная матрица размером n×n (в данной реализации используется единичная матрица I)
- **b** — вектор правых частей размером n (заполнен единицами: b = [1, 1, ..., 1]^T)
- **x** — искомый вектор решения размером n

**Входные данные:**

- Целое число n > 0 — размерность системы

**Выходные данные:**

- Целое число — округленная сумма компонент решения sum(x[i])

**Параметры метода:**

- Максимальное число итераций: 1000
- Точность сходимости: ε = 10⁻⁶
- Параметр релаксации: τ = 0.5

## 3. Описание последовательного алгоритма

Метод простой итерации основан на итерационной схеме:

x⁽ᵏ⁺¹⁾ = x⁽ᵏ⁾ - τ·(A·x⁽ᵏ⁾ - b) , где:

- **x⁽ᵏ⁾** — приближение решения на k-й итерации
- **τ** — параметр релаксации
- **A** — матрица системы
- **b** — вектор правых частей

**Алгоритм:**

1. **Инициализация:**
   - Начальное приближение: x⁽⁰⁾ = [0, 0, ..., 0]^T
   - Инициализация матрицы A как единичной: A[i][i] = 1
   - Вектор правых частей: b = [1, 1, ..., 1]^T

2. **Итерационный процесс** (для k = 0, 1, 2, ...):
   - Для каждой компоненты i:
     - Вычислить (Ax)[i] = Σⱼ(A[i][j] × x⁽ᵏ⁾[j])
     - Обновить: x⁽ᵏ⁺¹⁾[i] = x⁽ᵏ⁾[i] - τ × ((Ax)[i] - b[i])
   - Вычислить норму изменения: ||x⁽ᵏ⁺¹⁾ - x⁽ᵏ⁾||₂
   - Если ||x⁽ᵏ⁺¹⁾ - x⁽ᵏ⁾||₂ < ε, завершить
   - Если k ≥ max_iterations, завершить

3. **Постобработка:**
   - Вычислить сумму компонент решения: result = Σᵢ(x[i])
   - Округлить до целого числа


**Условие сходимости метода простой итерации:**

Для итерационного процесса вида **x⁽ᵏ⁺¹⁾ = x⁽ᵏ⁾ - τ(Ax⁽ᵏ⁾ - b)** сходимость гарантирована, если выполняется одно из условий:

1. **Спектральный критерий:** ρ(I - τA) < 1, где ρ(·) — спектральный радиус матрицы
2. **Достаточное условие:** ‖I - τA‖ < 1 для некоторой матричной нормы

**Для данной конкретной задачи (A = I):**
- Матрица итераций: **B = I - τI = (1 - τ)I**
- Спектральный радиус: ρ(B) = |1 - τ|
- При τ = 0.5: ρ(B) = 0.5 < 1   метод гарантированно сходится
- Скорость сходимости: ‖x⁽ᵏ⁾ - x*‖ ≤ 0.5ᵏ‖x⁽⁰⁾ - x*‖

Таким образом, для единичной матрицы метод сходится при любом 0 < τ < 2, а при τ = 0.5 обеспечивается оптимальная скорость сходимости.


**Обоснование выбора параметра релаксации τ = 0.5:**

В данной реализации выбран параметр τ = 0.5, что обеспечивает оптимальную скорость сходимости для системы с единичной матрицей:

1. **Общая формула сходимости:** Для метода итераций x⁽ᵏ⁺¹⁾ = x⁽ᵏ⁾ - τ(Ax⁽ᵏ⁾ - b) сходимость гарантирована при ρ(I - τA) < 1

2. **Для A = I:**
   - Матрица итераций: B = I - τI = (1 - τ)I
   - Собственные значения: λᵢ = 1 - τ
   - Спектральный радиус: ρ(B) = |1 - τ|

3. **Условие сходимости:** |1 - τ| < 1 ⇔ 0 < τ < 2

4. **Оптимальный выбор:** 
   - Для минимизации ρ(B) нужно минимизировать |1 - τ|
   - Минимум достигается при τ = 1, тогда ρ(B) = 0 → метод сходится за одну итерацию
   - Однако в данной реализации τ = 0.5 выбран как компромисс:
     * ρ(B) = 0.5 < 1 → гарантированная сходимость
     * Метод не становится слишком чувствительным к вычислительным погрешностям
     * Сохраняется устойчивость при возможных модификациях алгоритма

5. **Ожидаемое количество итераций:**
   - При ρ = 0.5: ‖x⁽ᵏ⁾ - x*‖ ≤ 0.5ᵏ‖x⁽⁰⁾ - x*‖
   - Для достижения точности ε = 10⁻⁶ требуется: 0.5ᵏ < 10⁻⁶ ⇒ k > log₂(10⁶) ≈ 20 итераций
   - Фактически метод сходится за ~10-15 итераций для данной системы

  Выбор τ = 0.5 обеспечивает:
- Гарантированную сходимость (ρ = 0.5 < 1)
- Хорошую скорость сходимости (геометрическая прогрессия со знаменателем 0.5)
- Устойчивость к вычислительным погрешностям
- Предсказуемое поведение алгоритма


**Сложность:** O(n² × iter), где iter — количество итераций до сходимости.

## 4. Схема распараллеливания (MPI)

Распараллеливание основано на декомпозиции по строкам матрицы A между MPI-процессами.

### 4.1 Распределение данных

**Топология:** Master-Worker с коллективными операциями

**Распределение строк:** Каждому процессу с рангом `rank` назначается блок строк:

- Базовое количество строк на процесс: `base_rows = n / size` (целочисленное деление)
- Дополнительные строки: первые `n % size` процессов получают по одной дополнительной строке
- Локальное количество строк: `local_rows = base_rows + (rank < n % size ? 1 : 0)`
- Смещение строк: `row_offset = Σ(rows_on_previous_procs)`

Пример распределения для n=10, size=3:
- `base_rows = 10 / 3 = 3`
- `extra_rows = 10 % 3 = 1` (только процесс 0 получает дополнительную строку)

┌─────────┬─────────────────┬─────────────┐
│ Процесс │ Диапазон строк  │ Лок. строк  │
├─────────┼─────────────────┼─────────────┤
│ 0       │ 0-3             │ 4           │
│ 1       │ 4-6             │ 3           │
│ 2       │ 7-9             │ 3           │
└─────────┴─────────────────┴─────────────┘

Пример распределения для n=10, size=4:
- `base_rows = 10 / 4 = 2`
- `extra_rows = 10 % 4 = 2` (процессы 0 и 1 получают по дополнительной строке)

┌─────────┬─────────────────┬─────────────┐
│ Процесс │ Диапазон строк  │ Лок. строк  │
├─────────┼─────────────────┼─────────────┤
│ 0       │ 0-2             │ 3           │
│ 1       │ 3-5             │ 3           │
│ 2       │ 6-7             │ 2           │
│ 3       │ 8-9             │ 2           │
└─────────┴─────────────────┴─────────────┘

Пример распределения для n=10, size=8:
- `base_rows = 10 / 8 = 1`
- `extra_rows = 10 % 8 = 2` (процессы 0 и 1 получают по дополнительной строке)

┌─────────┬─────────────────┬─────────────┐
│ Процесс │ Диапазон строк  │ Лок. строк  │
├─────────┼─────────────────┼─────────────┤
│ 0       │ 0-1             │ 2           │
│ 1       │ 2-3             │ 2           │
│ 2       │ 4-4             │ 1           │
│ 3       │ 5-5             │ 1           │
│ 4       │ 6-6             │ 1           │
│ 5       │ 7-7             │ 1           │
│ 6       │ 8-8             │ 1           │
│ 7       │ 9-9             │ 1           │
└─────────┴─────────────────┴─────────────┘


### 4.2 Схема коммуникации

На каждой итерации:

1. **Локальные вычисления:** Каждый процесс вычисляет новое приближение для своих строк:

local_x_new[i] = x[start_row + i] - τ × (Σⱼ(A_local[i][j] × x[j]) - b_local[i])


2. **Коллективная операция MPI_Allgatherv:**
- Собирает локальные результаты `local_x_new` от всех процессов в глобальный вектор `x_new`
- Все процессы получают полный обновленный вектор

3. **Проверка сходимости:**
- Каждый процесс вычисляет локальную норму разности
- MPI_Allreduce объединяет локальные нормы в глобальную
- Все процессы проверяют условие сходимости

4. **Финализация:**
- Каждый процесс вычисляет локальную сумму компонент решения
- MPI_Reduce суммирует локальные суммы на процессе 0
- MPI_Bcast распространяет итоговый результат на все процессы

## 5. Детали реализации

### 5.1 Структура кода

**Файлы:**

- `common/include/common.hpp` — общие типы и базовый класс Task
- `seq/include/ops_seq.hpp`, `seq/src/ops_seq.cpp` — последовательная реализация
- `mpi/include/ops_mpi.hpp`, `mpi/src/ops_mpi.cpp` — параллельная реализация
- `tests/functional/main.cpp` — функциональные тесты
- `tests/performance/main.cpp` — тесты производительности

**Основные классы:**

- `IvanovaPSimpleIterationMethodSEQ` — последовательная версия
- `IvanovaPSimpleIterationMethodMPI` — параллельная версия

**Основные методы:**

- `ValidationImpl()` — проверка входных данных (n > 0)
- `PreProcessingImpl()` — инициализация выходных данных
- `RunImpl()` — основной вычислительный процесс
- `PostProcessingImpl()` — проверка корректности результата

### 5.2 Вспомогательные функции (MPI)

```cpp
void ComputeDistribution(int n, int size, std::vector<int>& row_counts, 
                     std::vector<int>& row_displs,
                     std::vector<int>& matrix_counts,
                     std::vector<int>& matrix_displs);

void InitializeSystem(std::vector<double>& flat_matrix, std::vector<double>& b, int n);

void ComputeLocalProduct(const std::vector<double>& local_matrix,
                     const std::vector<double>& x,
                     const std::vector<double>& local_b,
                     std::vector<double>& local_x_new,
                     int local_rows, int start_row, int n, double tau);

void AllGatherVector(const std::vector<double>& local_x,
                  std::vector<double>& x_global,
                  const std::vector<int>& row_counts,
                  const std::vector<int>& row_displs);

bool CheckConvergenceAll(double local_diff, double epsilon);
```

### 5.3 Особенности реализации

**Хранение матрицы:**

- **В SEQ версии:** `std::vector<std::vector<double>>` (заменено на плоский массив в оптимизированной версии)
- **В MPI версии:** плоский массив `std::vector<double>` для эффективного распределения

**Коммуникационные операции:**

- `MPI_Scatterv` — начальное распределение данных
- `MPI_Allgatherv` — сбор обновленного вектора на каждой итерации
- `MPI_Allreduce` — проверка сходимости на каждой итерации
- `MPI_Reduce` + `MPI_Bcast` — финализация результата

**Обработка граничных случаев:**

- Если n ≤ 0, функция возвращает `false`
- Если число итераций достигло максимума, возвращается текущее приближение

## 6. Экспериментальное окружение

### 6.1 Аппаратное и программное обеспечение, компиляция и сборка

- **Модель процессора:** AMD Ryzen 7 5700U (8-ядерный процессор)
- **Архитектура:** Lucienne-U (Zen 2, x86-64)
- **Ядра/потоки:** 8 ядер, 16 потоков
- **Оперативная память:** 16 GB
- **Операционная система:** Windows 10
- **Тип системы:** Ноутбук (HP Laptop 15s-eq2034ur)

- **Компилятор:** g++ 13.3.0
- **Стандарт языка:** C++17
- **Среда разработки:** Visual Studio Code
- **Тип сборки:** Release
- **Система сборки:** CMake

### 6.2 Параметры тестирования

- **Размер задачи:** n = 2500
- **Количество процессов:** {1, 2, 4, 8, 10}
- **Режимы выполнения:** pipeline, task_run

## 7. Результаты и обсуждение

### 7.1 Проверка корректности

Корректность реализации проверялась следующим образом:

1. **Сравнение с аналитическим решением:**
   - Для системы Ix = b, где b = [1, 1, ..., 1]^T, точное решение x = [1, 1, ..., 1]^T
   - Сумма компонент решения должна быть равна n
   - Проверка: `output == n` (с учетом округления)

2. **Сравнение SEQ и MPI версий:**
   - Результаты последовательной и параллельной версий должны совпадать
   - Функциональные тесты проверяют эквивалентность результатов для различных n

3. **Тесты граничных случаев:**
   - Тесты для малых значений n (1, 2, 3)
   - Тесты для некорректных входных данных (n ≤ 0)
   - Проверка сохранения состояния

4. **Проверка сходимости:**
   - Метод сходится за количество итераций < 1000
   - Норма изменения вектора на последней итерации < ε = 10⁻⁶

**Результат:** Все функциональные тесты пройдены. Расхождение между SEQ и MPI версиями отсутствует.

### 7.2 Производительность

#### 7.2.1 Измерения времени выполнения

Результаты тестирования для размера задачи n = 2500:

**Режим task_run:**

| Процессов | SEQ (сек) | MPI (сек) | Ускорение MPI vs SEQ | Эффективность |
|-----------|-----------|-----------|----------------------|---------------|
| 1         | 0.967     | -         |-                     | -             |
| 2         | -         | 0.537     | 1.80                 | 90%           |
| 4         | -         | 0.345     | 2.80                 | 70%           |
| 8         | -         | 0.252     | 3.84                 | 48%           |
| 10        | -         | 0.303     | 3.19                 | 32%           |

**Режим pipeline:**

| Процессов | SEQ (сек) | MPI (сек) | Ускорение MPI vs SEQ | Эффективность |
|-----------|-----------|-----------|----------------------|---------------|
| 1         | 0.970     | -         |-                     | -             |
| 2         | -         | 0.538     | 1.80                 | 90%           |
| 4         | -         | 0.364     | 2.66                 | 67%           |
| 8         | -         | 0.263     | 3.69                 | 46%           |
| 10        | -         | 0.295     | 3.29                 | 33%           |

*Ускорение вычисляется как S = T_seq / T_mpi  
Эффективность вычисляется как E = S / N_proc × 100%*

#### 7.2.2 Анализ результатов

1. **Сравнение SEQ и MPI на 1 процессе:**
   - MPI версия немного медленнее SEQ (~5% для task_run, ~2% для pipeline)
   - Причина: накладные расходы MPI-инфраструктуры даже при использовании одного процесса

2. **Масштабируемость MPI:**

График ускорения (task_run):
4.0x ┤         ╭─╮
3.5x ┤       ╭─╯ ╰─
3.0x ┤     ╭─╯
2.5x ┤   ╭─╯
2.0x ┤ ╭─╯
1.5x ┼─╯
1.0x ┤
     1   2   4   8   10 (процессы)


3. **Ключевые наблюдения:**
- До 4 процессов: хорошая масштабируемость (эффективность > 67%)
- 4-8 процессов: умеренная масштабируемость (эффективность 46-48%)
- 10 процессов: значительное падение эффективности (32-33%)

4. **Сравнение режимов выполнения:**
- Режим pipeline в среднем на 5-10% быстрее task_run для MPI
- Для SEQ версии разница между режимами незначительна

5. **Узкие места:**
- Коммуникационная операция `MPI_Allgatherv` на каждой итерации
- При увеличении числа процессов растут накладные расходы на синхронизацию
- Для n=2500 объем вычислений на процесс уменьшается быстрее, чем снижаются коммуникационные затраты

## 8. Выводы

В рамках данной лабораторной работы была успешно реализована параллельная версия метода простой итерации для решения СЛАУ с использованием технологии MPI.

**Достигнутые результаты:**

1. Реализованы корректные SEQ и MPI версии алгоритма
2. Обеспечена идентичность результатов последовательной и параллельной реализаций
3. Достигнуто ускорение до 3.8x при использовании 8 процессов
4. Разработан комплекс функциональных тестов, проверяющих все граничные случаи

**Ограничения и узкие места:**

1. Коммуникационная операция `MPI_Allgatherv` на каждой итерации ограничивает масштабируемость
2. При большом количестве процессов накладные расходы на коммуникации преобладают над вычислительной работой
3. Для задачи размером n=2500 оптимальное количество процессов — 4-8

**Возможные улучшения:**

1. Увеличение размера задачи для лучшего соотношения вычисления/коммуникации
2. Использование асинхронных коммуникаций (MPI_Isend/MPI_Irecv)
3. Оптимизация SEQ версии для более справедливого сравнения
4. Реализация блочного распределения данных для уменьшения коммуникаций

## 9. Источники

1. Копченова Н.В., Марон И.А. *Вычислительная математика в примерах и задачах*. Лань, 2009.
2. Документация по курсу «Параллельное программирование» // Parallel Programming Course URL: https://learning-process.github.io/parallel_programming_course/ru/index.html (дата обращения: 15.12.2025).

## Приложение

### Основной вычислительный цикл (MPI)

```cpp
for (int iteration = 0; iteration < max_iterations; ++iteration) {
// Локальные вычисления
ComputeLocalProduct(local_matrix, x, local_b, local_x_new,
                  local_rows, start_row, n, tau);

// Сбор обновленного вектора со всех процессов
AllGatherVector(local_x_new, x_new, row_counts, row_displs);

// Проверка сходимости
double local_diff = ComputeLocalDiff(x_new, x, local_rows, start_row);
x.swap(x_new);

if (CheckConvergenceAll(local_diff, epsilon)) {
 break;
}
}

- Примечание: все процессы хранят полный вектор x и синхронно обновляют его на каждой итерации.
